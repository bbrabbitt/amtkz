<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="">
  <meta name="author" content="Amatsukazedaze">
  <meta name="keywords" content="">
  <title>pytorch&amp;caffe2 深度学习的魔法 CNN与图片分类网络 ~ hanayuki</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.1"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>hanayuki</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">Archives</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">Categories</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">Tags</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">About</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('/img/hanayuki.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期一, 四月 20日 2020, 1:23 下午
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    6.3k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      25 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <p>尽管一个能够识别手写数字的神经网络是非常有趣，但它的使用范围非常有限：只能识别数字，输入的数字必须是28*28的单通道灰度图像，而且识别准确率也并不是很高。出现这样的问题一部分原因是多层感知机的参数过多。假如要识别一张500万像素的图片，那么就需要至少500万个神经元放在输入层，而每一个神经元都需要连接到隐藏层中的神经元，导致整个神经网络计算复杂度过高，且大量的参数使梯度下降十分难进行，神经网络几乎不能收敛。另一方面，输入层的尺寸固定导致神经网络的可扩展性有限。因此，为了识别尺寸更大的图片人们提出了一种新的网络结构：卷积神经网络(Convolutional Neural Network, CNN)。</p>
<h1 id="卷积神经网络"><a class="markdownIt-Anchor" href="#卷积神经网络"></a> 卷积神经网络</h1>
<p>卷积神经网络中的卷积卷积是一个数学分析的概念，而神经网络中的卷积则指的是一种更加简单的运算：互相关运算。卷积的本质由于涉及到高等数学的知识在此不做讨论，我们只需了解神经网络中它是用来做什么的以及如何运算。</p>
<p>在卷积神经网络中有多种不同的层结构，最基本的包括包括卷积层，池化层和全连接层。这种网络结构是受猫的大脑结构的启发。猫的大脑在处理图像时会对图像进行加工，只提取出最重要的部分，使猫能够对环境快速反应。科学家因此设计出了卷积层。卷积层通过卷积核提取图像中最重要的特征，并去除不重要的像素以减少复杂度。卷积核是一个非常小的矩阵，通常尺寸是3*3，5*5或7*7，整个卷积层只需要训练一个卷积核就可以提取图像的特征，而且不受输入图像尺寸的影响，因此参数数量可以大大减少。下面以一个2*2的卷积核应用于一个3*3的输入图像举例卷积核是如何工作的。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnalign="left left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>9</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>6</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>8</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>5</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>7</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>8</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>4</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>6</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\left[\begin{array}{cc}
  1 &amp; 0\\
  1 &amp; 0\\
\end{array}\right] 
\left[\begin{array}{lll}
 9 &amp; 6 &amp; 8 \\
5 &amp; 7 &amp; 8 \\
4 &amp; 6 &amp; 1 \\
\end{array}\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.60004em;vertical-align:-1.55002em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">9</span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">5</span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">6</span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">7</span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">8</span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">8</span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>首先我们将卷积核放在左上角，然后计算卷积，即将对应位置上的数相乘然后相加，也就是1*9+0*6+1*5+0*7=14，因此输出矩阵的第一个值就是14。接下来将卷积核向右移动一格，计算出1*6+0*8+1*7+0*8=13。以此类推我们得出一个新的矩阵：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>14</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>13</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>9</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>13</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><annotation encoding="application/x-tex">\left[\begin{array}{cc}
14 &amp; 13 \\ 
9 &amp; 13 \\
\end{array}\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord">4</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">9</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord">3</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span></span></span></span></span></p>
<p>仅需要一个卷积核，我们便可以得到一个比原图像小的多的矩阵，并且这个矩阵提取了原图像中最重要的信息（假设卷积核训练效果良好），大大减少了需要的计算量。 如果一个卷积层中有两个以上的输入通道，那么卷积层中就会有对应数量的卷积核，每个通道都会和对应的卷积核进行互相关操作，得到的新矩阵会相加后输出。例如一个两通道的输入，那么就会有两个卷积核，每一个卷积核对应一个通道，得到两个不同的矩阵后相加输出。</p>
<p>由此可见，不管输入几个通道卷积层默认都只输出一个通道，那么如果要输出多个通道怎么办呢？只需要给卷积核增加通道就可以了。例如一个有两个输入，核的尺寸为2卷积层卷积核的形状是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mo>∗</mo><mn>2</mn><mo>∗</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2*2*2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span>，按照上述的计算方法最后只会有一个通道输出。因此如果想要有两个通道输出就得在每个输出通道上有两个卷积核，让卷积核的形状为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mo>∗</mo><mn>2</mn><mo>∗</mo><mn>2</mn><mo>∗</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2*2*2*2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span>。这里的顺序是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><msub><mi>t</mi><mi>c</mi></msub><mi>h</mi><mi>a</mi><mi>n</mi><mi>n</mi><mi>e</mi><mi>l</mi><mi>s</mi><mo>∗</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><msub><mi>t</mi><mi>c</mi></msub><mi>h</mi><mi>a</mi><mi>n</mi><mi>n</mi><mi>e</mi><mi>l</mi><mi>s</mi><mo>∗</mo><mi>k</mi><mi>e</mi><mi>r</mi><mi>n</mi><mi>e</mi><msub><mi>l</mi><mi>h</mi></msub><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>∗</mo><mi>k</mi><mi>e</mi><mi>r</mi><mi>n</mi><mi>e</mi><msub><mi>l</mi><mi>w</mi></msub><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">output_channels*input_channels*kernel_height*kernel_width</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">o</span><span class="mord mathdefault">u</span><span class="mord mathdefault">t</span><span class="mord mathdefault">p</span><span class="mord mathdefault">u</span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">n</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord mathdefault">p</span><span class="mord mathdefault">u</span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">n</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">n</span><span class="mord mathdefault">e</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">e</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">h</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">n</span><span class="mord mathdefault">e</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span><span class="mord mathdefault">t</span><span class="mord mathdefault">h</span></span></span></span>。这样在进行互相关运算时会先调用第1组卷积核为两个通道进行互相关运算得出一个新的矩阵，然后再调用第二组卷积核计算出另一个矩阵，然后将这两个矩阵连结成新的两个通道输出，就得到了有多个输出通道的卷积层。</p>
<p>一般来说卷积层后会有一个池化层，它的工作方式和卷积层差不多，有一个类似卷积核的池化窗口，但是它只会选择最大的一个数值或平均所有数值输出。上述例子中的矩阵经过一个2*2最大池化层时只有14会被输出而经过平均池化层则会只有12.25被输出。池化层的用途是减少卷积层对位置的敏感性。例如我们设计了一个提取边缘的神经网络，但在图像上边缘往往不会只有一两个像素宽，并且会在图像的各个角落出现，因此卷积层可能会在不同位置提取出大量重复的边缘特征。池化层通过提取最大值减少了这种重复的特征，提高神经网络的准确性。</p>
<p>我们可以发现卷积核和池化窗口的尺寸会直接影响卷积神经网络的性能。过大的卷积核不容易训练，而过小则会导致神经网络不能充分学习，因此卷积核和池化窗口的尺寸需要慎重选择。像这样需要人调整的神经网络参数被称作超参数，也就是计算机本身不能自己学习的参数。除了卷积核和池化窗口尺寸以外，卷积神经网络还有两个重要的超参数：填充(padding)和步长(stride)。当一张图像比神经网络输入层尺寸小或者我们需要让卷积层输入输出尺寸一样时，就需要人为地使图像尺寸增大，这个操作称为填充。在默认情况下，我们会让卷积核一次移动一个像素，但卷积核也可以一次移动更多个像素，而每一次卷积核要移动的距离则由步长来定义。要注意的是池化窗口一般情况下步长和池化窗口尺寸一致，例如一个2*2的池化窗口一般情况下会一次移动两个像素，因此每个池化窗口不会重叠，可以更高效地完成池化操作。</p>
<p>一个典型的卷积神经网络会有多个卷积层和池化层交替放置在减少输入尺寸的同时突出特征。卷积层和池化层提取图像特征后，这些特征将输入最后的全连接层进行实际的识别。此时进入全连接层的特征图像相当于是原图的精华。尽管全连接层的输入尺寸依旧是固定的，卷积层提取出的特征可以帮助全连接层更快地学习到识别图像需要的特征同时大大降低了计算的复杂度。</p>
<p>为了缩短训练时间我们使用MNIST的变种FashionMNIST来训练一个卷积神经网络。但卷积神经网络可以适应更大的图片，在下一节我们将使用<a href="https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz" target="_blank" rel="noopener">CINIC-10</a>训练一个更加强大的分类网络。</p>
<h1 id="使用pytorch构建网络"><a class="markdownIt-Anchor" href="#使用pytorch构建网络"></a> 使用pytorch构建网络</h1>
<h2 id="下载fashionmnist数据集"><a class="markdownIt-Anchor" href="#下载fashionmnist数据集"></a> 下载FashionMNIST数据集</h2>
<p>下载FashionMNIST数据集步骤和MNIST类似，只需要将所有的MNIST改成FashionMNIST</p>
<pre><code>import torch
from torchvision import datasets
from torchvision import transforms
#datasets.MNIST(加载路径，是否使用训练集（设为False导入测试集），
#文件不存在下载，数据集预处理函数）
train_loader = torch.utils.data.DataLoader(
	datasets.FashionMNIST('./FashionMNIST', train=True, download=True,transform=transforms.ToTensor()),
	batch_size=50, shuffle=True)
test_loader =  torch.utils.data.DataLoader(
    datasets.FashionMNIST('./FashionMNIST', train=False, download=True,transform=transforms.ToTensor()),
    batch_size=50, shuffle=True)
</code></pre>
<p>使用同样的方法查看一下数据集的大小</p>
<pre><code>images, labels = next(iter(train_loader))
images.size(),labels.size()
</code></pre>
<p>FashionMNIST数据集和MNIST是完全兼容的数据集，因此图像集和标签的大小和MNIST是一模一样的。</p>
<p>用同样的代码查看一下前十张图</p>
<pre><code>import matplotlib.pyplot as plt
%matplotlib inline
figs,axs = plt.subplots(1,10, figsize=(20, 20)) 
for ax,i,label in zip(axs,range(0,10),labels):
	ax.imshow(images[i].reshape(28,28),cmap=&quot;gray&quot;) 
	ax.set_title(label.item()) 
	ax.axes.get_xaxis().set_visible(False)
	ax.axes.get_yaxis().set_visible(False)
</code></pre>
<img src="/2020/04/20/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-CNN%E4%B8%8E%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/FashionMNIST_dataset.png" srcset="undefined" class="">
<h2 id="lenet-5"><a class="markdownIt-Anchor" href="#lenet-5"></a> LeNet-5</h2>
<p>接下来我们就可以构建一个CNN了。截至2020年人们对CNN的研究已经超过22年，其中诞生了很多经典的网络结构，而这次我们实现的是最早的一个CNN，Yann LeCun的LeNet-5。这里的5表示这个网络有5层，以卷积层-最大池化层-卷积层-最大池化层-全连接层排列，如下图。</p>
<img src="/2020/04/20/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-CNN%E4%B8%8E%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/lenet.png" srcset="undefined" class="">
<p>图上的每一个方框代表CNN网络的一层，具体结构如下</p>
<table>
<thead>
<tr>
<th>层名</th>
<th>输入形状</th>
<th>输出形状</th>
<th>核/窗口尺寸</th>
</tr>
</thead>
<tbody>
<tr>
<td>conv1</td>
<td>1*28*28</td>
<td>6*24*24</td>
<td>5*5</td>
</tr>
<tr>
<td>pool1</td>
<td>6*24*24</td>
<td>16*12*12</td>
<td>2*2</td>
</tr>
<tr>
<td>conv2</td>
<td>16*12*12</td>
<td>16*8*8</td>
<td>5*5</td>
</tr>
<tr>
<td>pool2</td>
<td>16*8*8</td>
<td>16*4*4</td>
<td>2*2</td>
</tr>
<tr>
<td>linear1</td>
<td>256</td>
<td>120</td>
<td>N/A</td>
</tr>
<tr>
<td>linear2</td>
<td>120</td>
<td>84</td>
<td>N/A</td>
</tr>
<tr>
<td>linear3</td>
<td>84</td>
<td>10</td>
<td>N/A</td>
</tr>
</tbody>
</table>
<p>注：原始的LeNet设计的输入形状为1*32*32，这里为了适应MNIST数据集减小了CNN的输入和输出的尺寸，但通道数和全连接层输入输出尺寸保持不变。计算卷积层输出形状参照以下公式：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">输</mi><mi mathvariant="normal">出</mi><mi mathvariant="normal">尺</mi><mi mathvariant="normal">寸</mi><mo>=</mo><mfrac><mrow><mi mathvariant="normal">输</mi><mi mathvariant="normal">入</mi><mi mathvariant="normal">尺</mi><mi mathvariant="normal">寸</mi><mo>−</mo><mi mathvariant="normal">卷</mi><mi mathvariant="normal">积</mi><mi mathvariant="normal">核</mi><mi mathvariant="normal">尺</mi><mi mathvariant="normal">寸</mi><mo>+</mo><mn>2</mn><mo>∗</mo><mi mathvariant="normal">填</mi><mi mathvariant="normal">充</mi><mi mathvariant="normal">尺</mi><mi mathvariant="normal">寸</mi><mo stretchy="false">(</mo><mi>p</mi><mi>a</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">步</mi><mi mathvariant="normal">长</mi><mo stretchy="false">(</mo><mi>S</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>d</mi><mi>e</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">输出尺寸=\frac{输入尺寸-卷积核尺寸+2*填充尺寸(padding)}{步长(Stride)}+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mord cjk_fallback">输</span><span class="mord cjk_fallback">出</span><span class="mord cjk_fallback">尺</span><span class="mord cjk_fallback">寸</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord cjk_fallback mtight">步</span><span class="mord cjk_fallback mtight">长</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord cjk_fallback mtight">输</span><span class="mord cjk_fallback mtight">入</span><span class="mord cjk_fallback mtight">尺</span><span class="mord cjk_fallback mtight">寸</span><span class="mbin mtight">−</span><span class="mord cjk_fallback mtight">卷</span><span class="mord cjk_fallback mtight">积</span><span class="mord cjk_fallback mtight">核</span><span class="mord cjk_fallback mtight">尺</span><span class="mord cjk_fallback mtight">寸</span><span class="mbin mtight">+</span><span class="mord mtight">2</span><span class="mbin mtight">∗</span><span class="mord cjk_fallback mtight">填</span><span class="mord cjk_fallback mtight">充</span><span class="mord cjk_fallback mtight">尺</span><span class="mord cjk_fallback mtight">寸</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>，再加上输出的通道数可以得到最终的输出形状。例如输入形状为1*28*28，卷积核尺寸为5*5，输出的尺寸就为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mn>28</mn><mo>−</mo><mn>5</mn><mo>+</mo><mn>2</mn><mo>∗</mo><mn>0</mn></mrow><mn>1</mn></mfrac><mo>+</mo><mn>1</mn><mo>=</mo><mn>24</mn></mrow><annotation encoding="application/x-tex">\frac{28-5+2*0}{1}+1=24</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">8</span><span class="mbin mtight">−</span><span class="mord mtight">5</span><span class="mbin mtight">+</span><span class="mord mtight">2</span><span class="mbin mtight">∗</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">4</span></span></span></span>，再定义输出6个通道，因此输出形状为6*24*24。</p>
<p>我们可以看到LeNet的每一层输入和输出大小不断减小，但通道数不断增加。每一层的大小减小表明CNN网络在不断地减少无用的信息，而通道数增加则是为了让网络有更多参数提取图像中的重要特征。在CNN中，每一个通道都代表了原图中的一个特征，通道数越多能够提取到的特征数量就越多，因此一些现代的CNN往往有384，512甚至1024个通道。</p>
<h2 id="构建网络"><a class="markdownIt-Anchor" href="#构建网络"></a> 构建网络</h2>
<p>从这里开始，我们遇到的神经网络层数就慢慢变多了。对于这种复杂的网络nn.Sequential就显得力不从心了。因此我们使用一个Python类来定义网络。这个类继承nn.Module类，在<code>__init__</code>里定义网络结构，另外还需要一个<code>forward</code>方法表明正向传播的路径。有了这两个信息pytorch就能自动补全反向传播的路径。下面来看看这个类是如何定义的。</p>
<pre><code>from torch import nn
from torch.nn.functional import relu
class LeNet5(nn.Module):
	def __init__(self):   
		super(LeNet5, self).__init__()
		self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
		self.pool1 = torch.nn.MaxPool2d(kernel_size=2,stride=2)
		self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)
		self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)
		self.linear1 = torch.nn.Linear(256, 120)   
		self.linear2 = torch.nn.Linear(120, 84)      
		self.linear3 = torch.nn.Linear(84, 10)      

	def forward(self, x):
		#使用relu函数而不是将激活函数定义在网络结构中
		x = relu(self.conv1(x))  
		x = self.pool1(x)
		x = relu(self.conv2(x))
		x = self.pool2(x)
		#将来自池化层的三维输出（通道数，高，宽）扁平化为向量输入进全连接层
		x = x.view(-1, 256)
		x = relu(self.linear1(x))
		x = relu(self.linear2(x))
		x = self.linear3(x)        
		return x
	
net = LeNet5()
</code></pre>
<p>为了确认我们定义的网络结构是否正确，我们需要查看一下网络结构。pytorch有两个非常方便的插件可以让我们查看网络的结构，分别是<a href="https://github.com/sksq96/pytorch-summary" target="_blank" rel="noopener">torchsummary</a>和<a href="https://github.com/szagoruyko/pytorchviz" target="_blank" rel="noopener">pytorchviz</a>。我们使用pip安装这两个插件<code>pip install torchviz torchsummary</code>。torchsummary可以将网络结构，输出尺寸，参数数量等信息用文字和表格的方式清晰地表示出来，首先我们先用这个插件查看一下我们网络的结构。</p>
<pre><code>from torchsummary import summary
#如果使用了带有CUDA的pytorch需要先将网络移动到显存中，否则会有CUDA张量和pytorch张量不兼容的问题
net.to(torch.device(&quot;cuda&quot;))
#第二个参数为输入形状，对于CNN来说形状通常为(通道数，高，宽)
summary(net, (1,28,28))

#使用CPU亦可。通常运行在CPU上报错信息会更加详细，如果遇到错误可以切换到CPU查看错误原因
net.to(torch.device(&quot;cpu&quot;))
summary(net, (1,28,28),device=&quot;cpu&quot;)	
</code></pre>
<img src="/2020/04/20/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-CNN%E4%B8%8E%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/summary.png" srcset="undefined" class="">
<p>可以看到整个网络的结构都已经清晰地显示出来了。从此处我们也可以看出全连接层和卷积层参数数量巨大的差异。</p>
<p>torchviz可以像caffe2一样将计算图画出来，要使用torchviz需要安装<a href="https://www.graphviz.org/download/" target="_blank" rel="noopener">graphviz</a>并加进系统的环境变量中。安装好后可以使用</p>
<pre><code>#torch.randn会随机生成一个给定形状的张量，这里的形状是（批量大小，通道，高，宽），如果是CUDA版的pytorch依然要将张量移动到显存中
x = torch.randn(1,1,28,28).to(torch.device(&quot;cuda&quot;))
y= net(x)
make_dot(y.mean(),params=dict(net.named_parameters()))
</code></pre>
<p>这个插件会自动生成一张网络的计算图，由于计算图包含了反向传播的部分所以显得比较复杂，在需要时可以参考使用。</p>
<img src="/2020/04/20/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-CNN%E4%B8%8E%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/computational_graph.png" srcset="undefined" class="">
<p>确认网络结构无误后，我们就可以训练了。向原来的初始化函数加一个对<code>nn.Conv2d</code>层的初始化，同样使用交叉熵损失函数和Adam优化算法。</p>
<pre><code>def init_weights(m):
	if type(m) == nn.Linear:
		nn.init.normal_(m.weight)
		m.bias.data.fill_(0.01)
	if type(m) == nn.Conv2d:
		nn.init.normal_(m.weight)
		m.bias.data.fill_(0.01)
</code></pre>
<p>net.apply(init_weights)<br />
criterion = torch.nn.CrossEntropyLoss()</p>
<pre><code>net.apply(init_weights)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(),lr=0.01)
</code></pre>
<p>训练时注意CNN不需要将图像变成向量传进网络，卷积层需要直接处理原图。</p>
<pre><code>#如果之前将网络移动到显存上查看结构，使用CPU训练时需要将网络移动回CPU
net.to(torch.device(&quot;cpu&quot;))
for epoch in range(5):
	loss = 0
	for _, data in enumerate(train_loader):
		features, labels = data
		optimizer.zero_grad()
		outputs = net(features)
		loss = criterion(outputs,labels)
		loss.backward()
		optimizer.step()
	
	#每运行完一代就显示训练效果
	print(&quot;epoch {}, loss {}&quot;.format(epoch+1,loss.data.item()))
</code></pre>
<p>由于CNN较之前的网络复杂，如果能使用CUDA加速的话可以使用如下训练代码。</p>
<pre><code>#将网络移动到显存上
net.to(torch.device(&quot;cuda&quot;))
#使用显存上的网络参数定义优化算法
optimizer = torch.optim.Adam(net.parameters(),lr=0.01)
for epoch in range(5):
	loss = 0
	for _, data in enumerate(train_loader):
		features, labels = data
		#将特征和标签移动到显存上
		features, labels = features.cuda(), labels.cuda()
		optimizer.zero_grad()
		outputs = net(features)
		loss = criterion(outputs,labels)
		loss.backward()
		optimizer.step()

	#每运行完一代就显示训练效果
	print(&quot;epoch {}, loss {}&quot;.format(epoch+1,loss.data.item()))
</code></pre>
<img src="/2020/04/20/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-CNN%E4%B8%8E%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/normal_loss.png" srcset="undefined" class="">
<p>奇怪的是在迭代5代后，损失依旧保持在1左右，而我们查看准确度则更是惨不忍睹</p>
<img src="/2020/04/20/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-CNN%E4%B8%8E%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/normal_accuracy.png" srcset="undefined" class="">
<p>作为对比，我们看看上一节编写的手写数字识别网络在使用FashionMNIST的准确率</p>
<img src="/2020/04/20/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-CNN%E4%B8%8E%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/fc_accuracy.png" srcset="undefined" class="">
<p>我们花费这么多精力设计出的比多层感知机复杂得多的卷积神经网络难道根本就一无是处？答案显然是否定的，我们的CNN训练结果如此糟糕和很多方面都有联系，而其中一个方面就是我们使用的初始化函数。到目前为止，我们都在沿用最简单的使用标准正态分布生成随机数的方法。这种方法的问题是容易生成过大或者过小的初始化参数。尽管正态分布生成的随机数落在一个标准差之间的概率最大，但落在两端的概率在动辄上万，甚至上亿个参数的神经网络里是无法被忽视的。那么过大或过小的参数会带来什么问题呢？这会对神经网络反向传播的性能有很大影响。简单地说，这会造成梯度爆炸和梯度消失两个不同的问题。</p>
<h2 id="一点数学"><a class="markdownIt-Anchor" href="#一点数学"></a> 一点数学</h2>
<p><em>以下内容包含一点高等数学，如对数学无感可跳过看结论</em></p>
<p>还记得我们之前提到过反向传播使用微分修正参数吗？大致来说反向传播是一个求偏导的过程。假设我们有一个两层，每层只有一个神经元的网络，每一层上有一个激活函数，也就是网络结构如下</p>
<p><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="524px" height="212px" viewBox="73 115 427 173" preserveAspectRatio="xMidYMid meet" ><rect id="svgEditorBackground" x="0" y="0" width="1050" height="570" style="fill:none; stroke: none;"/><circle id="e1_circle" cx="318.77227783203125" cy="158.90536499023438" style="fill:none;stroke:black;stroke-width:1px;" r="53.8144961883" transform="matrix(0.419978 0 0 0.419978 104.372 123.513)"/><circle id="e14_circle" cx="97.80815124511719" cy="159.38104248046875" style="fill:none;stroke:black;stroke-width:1px;" r="53.8144961883" transform="matrix(0.419978 0 0 0.419978 104.372 123.513)"/><circle id="e2_circle" cx="543.5452270507812" cy="155.09463500976562" style="fill:none;stroke:black;stroke-width:1px;" r="53.8144961883" transform="matrix(0.419978 0 0 0.419978 104.372 123.513)"/><path d="M168,190h48" style="fill:none;stroke:black;stroke-width:1px;" id="e5_pathH"/><path d="M262,190h48" style="fill:none;stroke:black;stroke-width:1px;" id="e2_pathH"/><text style="fill:black;font-family:Arial;font-size:20px;" x="133" y="187" id="e6_texte"></text><text style="fill:black;font-family:Arial;font-size:20px;" x="94.95985412597656" y="234.56033325195312" id="e7_texte" transform="matrix(0.912421 0 0 0.912421 53.4807 -17.5701)">x</text><text style="fill:black;font-family:Arial;font-size:20px;" x="177" y="182" id="e8_texte">w1</text><text style="fill:black;font-family:Arial;font-size:20px;" x="232" y="196" id="e9_texte">f1</text><text style="fill:black;font-family:Arial;font-size:20px;" x="269" y="184" id="e10_texte">w2</text><text style="fill:black;font-family:Arial;font-size:20px;" x="326" y="195" id="e11_texte">f2</text><path d="M356,190h48" style="fill:none;stroke:black;stroke-width:1px;" id="e1_pathH"/><text style="fill:black;font-family:Arial;font-size:20px;" x="428" y="193" id="e14_texte"></text><text style="fill:black;font-family:Arial;font-size:20px;" x="410" y="188" id="e15_texte"></text><circle id="e3_circle" cx="765.462890625" cy="155.57034301757812" style="fill:none;stroke:black;stroke-width:1px;" r="53.8144961883" transform="matrix(0.419978 0 0 0.419978 104.372 123.513)"/><text style="fill:black;font-family:Arial;font-size:20px;" x="176" y="253" id="e16_texte" transform="matrix(1 0 0 1 244 -59)">y</text></svg></p>
<p>从图中我们可以看出这个网络的正向传播可以用如下等式表示</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><msub><mi>f</mi><mn>2</mn></msub><mo stretchy="false">(</mo><msub><mi>w</mi><mn>2</mn></msub><mo>∗</mo><mo stretchy="false">(</mo><msub><mi>f</mi><mn>1</mn></msub><mo stretchy="false">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>∗</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = f_2(w_2*(f_1(w_1*x)))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<p>而反向传播则是用误差对其中一个权重求偏导来对权重进行更新，例如对<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">w_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>求偏导得</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>w</mi><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mn>1</mn></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>f</mi><mn>2</mn></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>f</mi><mn>2</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>f</mi><mn>1</mn></msub></mrow></mfrac><msub><mi>w</mi><mn>2</mn></msub><mo>⋅</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>f</mi><mn>1</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mn>1</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\Delta w = \frac{\partial loss}{\partial w_1} = \frac{\partial loss}{\partial f_2} \cdot \frac{\partial f_2}{\partial f_1}w_2 \cdot \frac{\partial f_1}{\partial w_1}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">Δ</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.20744em;vertical-align:-0.8360000000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.3139999999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord mathdefault">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8360000000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.25188em;vertical-align:-0.8804400000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord mathdefault">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804400000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.25188em;vertical-align:-0.8804400000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804400000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.20744em;vertical-align:-0.8360000000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.3139999999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8360000000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>然后再将<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">\Delta w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">Δ</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span>加到原权重上</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>=</mo><msub><mi>w</mi><mn>1</mn></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">w_1 = w_1 + \Delta w
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">Δ</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span></span></p>
<p>在这个过程中权重<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">w_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>需要和激活函数的导数相乘。如果激活函数是sigmoid函数，而sigmoid函数的导数如下图</p>
<img src="/2020/04/20/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-CNN%E4%B8%8E%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/sigmoid_D.png" srcset="undefined" class="">
<p>由于sigmoid的导数值域为(0,0.25]，当<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">w_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>初始化的值过小，在经过反向传播后<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">\Delta w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">Δ</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span>就会趋近于0，导致<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">w_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>得不到更新。同理当<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">w_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>初始化值过大导致导数乘<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">w_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的值超过1，反向传播后得到的梯度会越来越大直至超过计算机的表示范围。随着网络层数增加，在反向传播时需要相乘的权重也会变多。在其中如果有多个权重初始化值不在理想范围就会导致最后得到的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">\Delta w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">Δ</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span>值过大或过小，使神经网络权重更新不稳定以至于无法收敛。需要注意的是梯度爆炸和梯度消失的原因有很多方面，例如从这个反向传播等式中我们也可以看出如果激活函数的导数过小梯度也会消失，这就是之前讲到的sigmoid会导致神经网络学习缓慢的原因。</p>
<p><strong>结论：使用正态分布初始化网络会产生过大或过小的权重参数，当网络变深时这些参数会使网络权重更新不稳定以至于无法收敛。</strong></p>
<p>因此，人们提出了两种新的初始化方法：Xavier初始化和He初始化。在这里我们不会深入讨论这两个初始化的细节，只需要知道Xavier初始化是针对sigmoid和tanh激活函数开发，而He初始化是针对ReLU激活函数对Xavier初始化提出的改善就可以了。</p>
<p>那么，我们修改一下权重的初始化函数，然后重新训练CNN看看结果如何。由于我们使用的是ReLU激活函数，我们选择He初始化函数进行初始化</p>
<pre><code>def init_weights(m):
	if type(m) == nn.Linear:
		#在pytorch中He初始化被称作kaiming_uniform_或kaiming_normal_，这是因为He初始化论文作者名为何恺明博士
		#uniform和normal指的是使用均匀分布还是正态分布版本的He初始化，在这里我们选择均匀分布
		nn.init.kaiming_normal_(m.weight)
		m.bias.data.fill_(0.01)
	if type(m) == nn.Conv2d:
		nn.init.kaiming_normal_(m.weight)
		m.bias.data.fill_(0.01)
		
net.apply(init_weights)
</code></pre>
<p>然后我们再来训练一次看看结果如何</p>
<img src="/2020/04/20/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-CNN%E4%B8%8E%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/he_loss.png" srcset="undefined" class="">
<img src="/2020/04/20/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-CNN%E4%B8%8E%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/he_accuracy.png" srcset="undefined" class="">
<p>可以看到效果立竿见影，识别的准确率已经超过了全连接网络，达到了86.27%，这说明使用CNN确实会对图像识别有很大改善。</p>
<h1 id="使用caffe2构建网络"><a class="markdownIt-Anchor" href="#使用caffe2构建网络"></a> 使用caffe2构建网络</h1>
<p>由于caffe2官方没有提供FashionMNIST数据集，我们需要自己制作一个可以用在caffe2的FashionMNIST数据集。由于MNIST数据集和FashionMNIST数据集完全兼容，我们可以使用caffe2提供的一个小工具<code>make_mnist_db</code>制作数据集。首先我们需要从FashionMNIST的<a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" rel="noopener">github</a>上下载数据集的原始文件。分别为<em>train-images-idx3-ubyte.gz</em>，<em>train-labels-idx1-ubyte.gz</em>，<em>t10k-images-idx3-ubyte.gz</em>和<em>t10k-labels-idx1-ubyte.gz</em>。下载解压后调用<code>make_mnist_db</code>生成LevelDB或LMDB格式的数据集。<code>make_mnist_db</code>位于pytorch编译目录的build\bin文件夹中。在Windows下如果遇到找不到nvtoolsext64_1.dll的问题将anaconda3\pkgs\cudatoolkit-*\Library\bin加入系统的PATH环境变量再重试一次。</p>
<pre><code>#--db参数可以选择lmdb或leveldb --image_file和--lavel_file分别是图像和标签文件位置
#生成训练集
make_mnist_db --channel_first true --db lmdb --image_file path/to/train-images-idx3-ubyte --label_file path/to/train-labels-idx1-ubyte --output_file output/fashionmnist-train-nchw-lmdb
#生成测试集
make_mnist_db --channel_first true --db lmdb --image_file path/to/t10k-images-idx3-ubyte --label_file path/to/t10k-labels-idx1-ubyte --output_file output/fashionmnist-test-nchw-lmdb
</code></pre>
<p>生成后我们就可以用同样的加载代码载入数据集了</p>
<pre><code>from caffe2.python import brew, core, model_helper, optimizer, workspace, net_drawer	
train_model = model_helper.ModelHelper(&quot;train_model&quot;,arg_scope={&quot;order&quot;:&quot;NCHW&quot;})
#注意修改文件位置，如果上一步使用LevelDB格式生成数据集这里的db_type也要改成对应的leveldb
features_uint8, labels = train_model.TensorProtosDBInput([],[&quot;features_uint8&quot;,&quot;labels&quot;],batch_size=50,db=&quot;path/to/fashionmnist-train-nchw-lmdb&quot;,db_type=&quot;lmdb&quot;)
features = train_model.Cast(features_uint8, &quot;features&quot;, to=core.DataType.FLOAT)
features = train_model.Scale(features,features,scale=float(1/256))
features = train_model.StopGradient(features,features)
</code></pre>
<p>接下来我们就要构建一个LeNet-5网络了</p>
<pre><code>#weight_init定义使用的初始化函数，我们依然使用He初始化初始化权重
#由于何恺明是在微软亚洲研究院(MSRA)研究出的He初始化，caffe2中He初始化被称作MSRAFill
conv1 = brew.conv(train_model, features, &quot;conv1&quot;,dim_in=1,dim_out=6,weight_init=(&quot;MSRAFill&quot;,{}),kernel=5)
relu1 = brew.relu(train_model, conv1, &quot;relu1&quot;)
pool1 = brew.max_pool(train_model, relu1, &quot;pool1&quot;,kernel=2,stride=2)
conv2 = brew.conv(train_model, pool1, &quot;conv2&quot;, dim_in=6,dim_out=16, weight_init=(&quot;MSRAFill&quot;,{}),kernel=5)
relu2 = brew.relu(train_model,conv2,&quot;relu2&quot;)
pool2 = brew.max_pool(train_model,relu2,&quot;pool2&quot;,kernel=2,stride=2)
fc1 = brew.fc(train_model,pool2,&quot;fc1&quot;,dim_in=16*4*4,dim_out=120,weight_init=(&quot;MSRAFill&quot;,{}))
relu3 = brew.relu(train_model,fc1,&quot;relu3&quot;)
fc2 = brew.fc(train_model,relu3,&quot;fc2&quot;,dim_in=120,dim_out=84,weight_init=(&quot;MSRAFill&quot;,{}))
relu4 = brew.relu(train_model,fc2,&quot;relu4&quot;)
fc3 = brew.fc(train_model,relu4,&quot;fc3&quot;,dim_in=84,dim_out=10,weight_init=(&quot;MSRAFill&quot;,{}))
softmax = brew.softmax(train_model,fc3,&quot;softmax&quot;)
</code></pre>
<p>同样还有交叉熵函数</p>
<pre><code>crossEntropy = train_model.LabelCrossEntropy([softmax,labels],&quot;crossEntropy&quot;)
loss = train_model.AveragedLoss(crossEntropy, &quot;loss&quot;)
</code></pre>
<p>确认计算图</p>
<pre><code>from IPython import display
graph = net_drawer.GetPydotGraph(train_model.Proto().op,&quot;train&quot;,rankdir=&quot;LR&quot;)
display.Image(graph.create_png(),width=800)
</code></pre>
<img src="/2020/04/20/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-CNN%E4%B8%8E%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/lenet_graph.png" srcset="undefined" class="">
<p>下面使用同样的Adam优化算法</p>
<pre><code>train_model.AddGradientOperators([loss])
optimizer.build_adam(
    train_model,
    base_learning_rate=0.001,
    policy=&quot;step&quot;,
    stepsize=1,
    gamma=0.999,
)
</code></pre>
<p>我们将整个数据集迭代5次。由于每个小批量大小是50，FashionMNIST的训练集大小是50000，我们需要迭代5000次。</p>
<pre><code>workspace.RunNetOnce(train_model.param_init_net)
workspace.CreateNet(train_model.net)
total_iters = 5000
loss = [0 for i in range(5000)]

for i in range(total_iters):
	workspace.RunNet(train_model.net.Proto().name)
	loss[i] = workspace.FetchBlob('loss')
loss
</code></pre>
<img src="/2020/04/20/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-CNN%E4%B8%8E%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/caffe2_loss.png" srcset="undefined" class="">
<img src="/2020/04/20/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-CNN%E4%B8%8E%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/caffe2_plot.png" srcset="undefined" class="">
<p>最后进行测试，注意要将init_params设成False，防止测试网络重新初始化参数。</p>
<pre><code>test_model = model_helper.ModelHelper(&quot;test_model&quot;,arg_scope={&quot;order&quot;:&quot;NCHW&quot;},init_params=False) 
features_test_uint8, labels_test = test_model.TensorProtosDBInput([],[&quot;features_test_uint8&quot;,&quot;labels_test&quot;],batch_size=200,db=&quot;fashionmnist-test-nchw-lmdb&quot;,db_type=&quot;lmdb&quot;)
features_test = test_model.Cast(features_test_uint8, &quot;features_test&quot;, to=core.DataType.FLOAT)
features_test = test_model.Scale(features_test,features_test,scale=float(1/256))
features_test = test_model.StopGradient(features_test,features_test)
conv1 = brew.conv(test_model, features_test, &quot;conv1&quot;,dim_in=1,dim_out=6,kernel=5)
relu1 = brew.relu(test_model, conv1, &quot;relu1&quot;)
pool1 = brew.max_pool(test_model, relu1, &quot;pool1&quot;,kernel=2,stride=2)
conv2 = brew.conv(test_model, pool1, &quot;conv2&quot;, dim_in=6,dim_out=16,kernel=5)
relu2 = brew.relu(test_model,conv2,&quot;relu2&quot;)
pool2 = brew.max_pool(test_model,relu2,&quot;pool2&quot;,kernel=2,stride=2)
fc1 = brew.fc(test_model,pool2,&quot;fc1&quot;,dim_in=16*4*4,dim_out=120)
relu3 = brew.relu(test_model,fc1,&quot;relu3&quot;)
fc2 = brew.fc(test_model,relu3,&quot;fc2&quot;,dim_in=120,dim_out=84)
relu4 = brew.relu(test_model,fc2,&quot;relu4&quot;)
fc3 = brew.fc(test_model,relu4,&quot;fc3&quot;,dim_in=84,dim_out=10)
softmax = brew.softmax(test_model,fc3,&quot;softmax&quot;)
accuracy = brew.accuracy(test_model, [softmax,labels_test],&quot;accuracy&quot;)
</code></pre>
<p>查看测试结果</p>
<pre><code>import numpy as np
workspace.RunNetOnce(test_model.param_init_net)
workspace.CreateNet(test_model.net)
total_iters = 50
accuracy = np.zeros(50)

for i in range(total_iters):
	workspace.RunNet(test_model.net)
	accuracy[i] = workspace.FetchBlob(&quot;accuracy&quot;)
plt.plot(accuracy)
print(&quot;test accuracy:{}&quot;.format(accuracy.mean()))
</code></pre>
<p>至此一个简单的CNN网络就搭建完成了，接下来几节我们将认识更多更加复杂的CNN结构，并使用CINIC-10进行训练。</p>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">神经网络</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/pytorch">pytorch</a>
                
                  <a class="hover-with-bg" href="/tags/caffe2">caffe2</a>
                
                  <a class="hover-with-bg" href="/tags/CNN">CNN</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
      <br><br>
      
      
  <div id="vcomments" style="width: 90%; margin: 0 auto;"></div>
  <script defer src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script defer src="//unpkg.com/valine/dist/Valine.min.js"></script>

  <script>
    var notify = 'false' === true;
    var verify = 'true' === true;
    var oldLoad = window.onload;
    window.onload = function () {
      new Valine({
        el: '#vcomments',
        notify: notify,
        verify: verify,
        app_id: "jY5stlz5eNHmsmr1p2wsi53q-MdYXbMMI",
        app_key: "R0kCXXt8GsmjHF9Nug1fSRWp",
        placeholder: "说点什么",
        avatar: "retro",
        meta: ['nick', 'mail', 'link'],
        pageSize: "10",
      });
      oldLoad && oldLoad();
    };
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://valine.js.org" target="_blank" rel="nofollow noopener noopener">comments
      powered by Valine.</a></noscript>


    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>


  <script src="/js/lazyload.js" ></script>



  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "pytorch&caffe2 深度学习的魔法 CNN与图片分类网络&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>



  

  
    <!-- KaTeX -->
    <link rel="stylesheet" href="/lib/katex/katex.min.css"  >
  





</body>
</html>
