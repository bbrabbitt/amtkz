<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="">
  <meta name="author" content="Amatsukazedaze">
  <meta name="keywords" content="">
  <title>pytorch&amp;caffe2~深度学习的魔法~使用CINIC-10训练更多网络 ~ hanayuki</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.1"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>hanayuki</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">Archives</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">Categories</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">Tags</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">About</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('/img/hanayuki.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期六, 五月 2日 2020, 10:36 晚上
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    9.2k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      39 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <p>上一节我们学习了CNN的基本结构，但是LeNet-5在FashionMNIST数据集上表现也不是非常出色，对于现实生活中更加复杂的图片LeNet-5就更是力不从心了。因此人们提出了更多复杂的卷积神经网络。本节中我们介绍几个经典的设计，分别是进一步加深LeNet-5的AlexNet，串联多个小网络的NiN，和目前大量CNN的基础残差网络ResNet。由于这一节的网络更加强大，为了有区分度我们使用CINIC-10数据集训练。</p>
<h1 id="cinic-10数据集"><a class="markdownIt-Anchor" href="#cinic-10数据集"></a> CINIC-10数据集</h1>
<p><a href="https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz" target="_blank" rel="noopener">CINIC-10</a>是一个CIFAR-10和Imagenet结合的数据集。CIFAR-10是MIT的TinyImages数据集的节选。TinyImages有着8000万张32*32的小图片。由于这个数据集过于庞大，多伦多大学的研究人员从中节选出了和MNIST类似，10个分类，60000张图片的小数据集CIFAR-10。而ImageNet则是另一个庞大的图像数据集，其中包含了超过1400万张的全尺寸图像，并且每一个图像都有对应的分类。由于这两个数据集都是收集了现实生活中的彩色照片，这些数据集相比于MNIST更具有实用性。ImageNet还曾举办CNN界最具竞争性的图像识别大赛ILSVRC，从2010到2017的7届比赛中诞生了很多经典的网络设计。本节中的AlexNet和ResNet便分别是2012和2015届ILSVRC的冠军，而受NiN启发的GoogLeNet则是2014届ILSVRC冠军。</p>
<p>由于ImageNet规模太大，就算是全部降采样成32*32的图片整个数据集也要有6GB左右，并且降采样后丢失了很多信息。而CIFAR-10如今又显得太小，对于一些复杂的网络来说不够有区分度。因此研究人员就提出了CINIC-10数据集。它是CIFAR-10和ImageNet的结合。爱丁堡大学的研究人员从ImageNet中挑选了和CIFAR-10的分类相关的图像整合在一起，形成了一个有270000张图像的数据集，因此相比CIFAR-10更能区分神经网络的表现，同时没有ImageNet数据集那么庞大，更加容易训练。</p>
<p>在下载CINIC-10后我们看看里面有什么</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/cinic-10.png" srcset="undefined" class="">
<p>可以看到里面有3个文件夹train,test和valid。train和test我们已经接触过很多次了，也就是训练集和测试集。那么valid是什么呢？valid是validate的缩写，也就是验证集。验证集的作用是为我们调参提供参考。上一节讲到神经网络有一些超参数需要我们不断调试使网络达到最佳状态。之前我们直接根据训练集的损失和测试集的准确率来判断我们选择的超参数是不是最优的，但是我们也提到过测试集是绝对不能参与训练过程的，因为它的作用是为神经网络提供一个客观的性能参考。如果我们根据测试集调整超参数，那么相当于测试集也参与到了训练过程中，这样测试集的可参考性就降低了。为了能够在不使用测试集的情况下衡量神经网络的性能，人们提出了验证集的概念。在神经网络训练结束后，我们可以使用验证集先测试一下网络大致的性能，然后根据验证集的准确度调整超参数，在多次测试后选取在验证集上最优的网络放到测试集上测试最终的效果。验证集不一定是一个独立的数据集，由于ImageNet数据量很大，我们可以选择一部分图像作为验证集。而在MNIST这样没有特意分出验证集的数据集上，我们可以使用两种不同的方法分出验证集。一种是直接从训练集中分出一部分（例如10000张图像）用作验证，剩余40000张训练。而另一种被称作K-fold交叉验证法，简单来说就是把50000张图像分成K份（例如10份），每次选择9份训练，剩下一份验证，循环10次后得出一个平均的结果。由于数据集较难收集，这种方法能够更加高效地利用数据集。</p>
<p>CINIC-10的训练集，测试集和验证集的大小一样，都是90000张图，每个类别有9000张。打开任意一个数据集的文件夹可以看到里面有10个分类，分别是飞机(airplane)，机动车(automobile)，鸟(bird)，猫(cat)，鹿(deer)，狗(dog)，青蛙(frog)，马(horse)，船(ship)和卡车(truck)。这些就是我们训练时要用到的标签了。</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/labels.png" srcset="undefined" class="">
<p>接着打开一个标签文件夹，可以看到里面的图片有两类，一类的前缀是cifar10，这些就是属于原来的CIFAR-10的数据集的图片。</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/CINIC-CIFAR-10.png" srcset="undefined" class="">
<p>而另一类图片则是有nxxxxxxxx的编号的前缀，这些图片来自ImageNet，而这串数字就是ImageNet的标签编号。</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/CINIC-ImageNet.png" srcset="undefined" class="">
<p>我们可以把这串编号输入进ImageNet的官网查询一下。将这串编号放在http://imagenet.stanford.edu/synset?wnid=的wnid=的后面，例如http://imagenet.stanford.edu/synset?wnid=n03335030，打开来就可以看到这个图片对应的类别是战斗机。</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/ImageNet-label.png" srcset="undefined" class="">
<p>除了这三个文件夹外，我们还能看到有3个文件，第一个是imagenet-contributors.csv，这个文件里就记录了来自ImageNet的图片的编号，所在的数据集和标签。第二个是README.md，也就是关于这个数据集的一些信息。最后一个是CIFAR-10的分类与ImageNet的对应关系。由于ImageNet的分类比CIFAR-10的分类要细的多，通过这个文件可以找到之间的对应关系。例如CIFAR-10的飞机分类就包含了ImageNet的水上飞机，喷气式飞机，战斗机等多个分类。</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/synsets-to-cifar-10-classes.png" srcset="undefined" class="">
<p>由于CINIC-10包含了两个不同数据集的数据，研究人员可以通过这两个文件和文件名提取出两个不同的数据集并进行对比，使这个数据集更加实用。</p>
<h1 id="使用pytorch加载数据集"><a class="markdownIt-Anchor" href="#使用pytorch加载数据集"></a> 使用pytorch加载数据集</h1>
<p>由于pytorch没有能自动下载CINIC-10的工具类，我们需要自己将CINIC-10数据集加载进来。当然pytorch已经准备了一个能加载任何图片的数据加载类<code>ImageFolder</code>，它能加载的格式是label/image.png，也正是CINIC-10的格式，因此我们能直接用这个工具类加载CINIC数据集后，再调用DataLoader生成pytorch需要的小批量加载器。</p>
<pre><code>import torch
from torchvision import datasets, transforms

image_transforms = transforms.Compose(
    [transforms.Resize((128,128)),
    transforms.ToTensor(),
    transforms.Normalize(mean = [0.47889522, 0.47227842, 0.43047404],
                        std = [0.24205776, 0.23828046, 0.25874835])]
)
train_loader = datasets.ImageFolder(&quot;CINIC-10/train&quot;,transform = image_transforms)
valid_loader = datasets.ImageFolder(&quot;CINIC-10/valid&quot;, transform = image_transforms)
test_loader = datasets.ImageFolder(&quot;CINIC-10/test&quot;, transform = image_transforms)
train_loader = torch.utils.data.DataLoader(train, batch_size=200, shuffle=True)
valid_loader = torch.utils.data.DataLoader(valid, batch_size=200, shuffle=True)
test_loader = torch.utils.data.DataLoader(test, batch_size=200, shuffle=True)
</code></pre>
<p>这里出现了三个新的函数<code>transforms.Compose</code>，<code>transforms.Resize</code>和<code>transforms.Normalize</code>。Compose函数很好理解，它只是将很多个预处理函数串联起来依次执行。而Resize顾名思义是将原来的32*32的图像放大成128*128的图像，这是因为本节的神经网络大多是为ImageNet设计的，本身设计的输入大小很大。为了兼顾训练速度和网络结构我们将它折中放大到128*128。而Normalize函数则是我们没有遇到过的一个新的预处理函数。它的作用是将图像归一化。归一化这个概念其实在手写数字识别网络的caffe2实现里就已经遇到过，只是当时并没有提到这个名词。归一化简单来说就是将图像特征更加集中，便于神经网络训练。在之前训练caffe2时我们做了最简单的归一化，也就是给每个值除以225。因为MNIST的特征已经很集中的出现在中间，所以这样的归一化就足够了。但CINIC-10的图像都是彩色图像，而且特征可能在图像的任何位置，我们需要使用标准化处理(Standarization)。<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>标准化的公式是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{x-\mu}{\sigma}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.199439em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.854439em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">σ</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">μ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，也就是正态分布算z值的公式。对于彩色图像来说每个通道都需要归一化，因此这里有三个不同的平均值和标准差。这里的平均值和标准差来自README.md。</p>
<p>读取数据后我们可以用如下代码查看图像</p>
<pre><code>classes = ('airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck')
images, labels = next(iter(train_loader))
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
mean = (0.47889522, 0.47227842, 0.43047404)
std = (0.24205776, 0.23828046, 0.25874835)
figs,axs = plt.subplots(1,10, figsize=(20, 20)) 
for ax,i,label in zip(axs,range(0,10),labels):
    for t,m,s in zip(images[i],mean,std):
        t.mul_(s).add_(m)
    ax.imshow(images[i].permute(1,2,0)) 
    ax.set_title(classes[label]) 
    ax.axes.get_xaxis().set_visible(False)
    ax.axes.get_yaxis().set_visible(False)
</code></pre>
<p>由于我们在导入数据的时候进行了标准化处理，在使用matplotlib查看图像时要将图像还原成原本的模样。还原的方式也很简单，就是将标准化的操作反过来，也就是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>∗</mo><mi>σ</mi><mo>+</mo><mi>μ</mi></mrow><annotation encoding="application/x-tex">x*\sigma+\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.46528em;vertical-align:0em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">μ</span></span></span></span>。同样我们需要在每一个通道上调用pytorch张量的<code>mul_</code>和<code>add_</code>方法还原这个矩阵。像这样名字后带下划线的方法都是就地操作方法，这些方法会直接改变张量本身而不是返回一个新的张量，有助于减少内存使用。由于matplotlib的implot函数显示彩色图像时要求把通道数放在第三维，而pytorch导入的图像通道数通常是在第一维，因此我们使用pytorch张量的<code>permute</code>方法，这个方法可以更换张量各个维的位置，例如<code>image[i].permute(1,2,0)</code>就是将第一维放在最后，第二第三维变成第一第二维。注意pytorch张量的维下标也是从0开始的。最后pytorch会自动将标签转换为一个索引数字，具体哪个数字对应哪个标签可以用数据集的<code>class_to_idx</code>属性确认。例如</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/class_to_idx.png" srcset="undefined" class="">
<p>上面使用的classes也正是对应的这个字典写成的。如果不确定自己的数据集对应的索引和上文的classes元组是不是一致，可以使用这个属性查看。在加载好数据集后，我们就可以来构建网络了。</p>
<h1 id="alexnet"><a class="markdownIt-Anchor" href="#alexnet"></a> AlexNet</h1>
<p>在上一节中我们发现CNN越深，也就是通道数越多，能够学习到的特征就越多，这也是深度学习名称的由来。AlexNet就是在LeNet的基础上进一步加深了网络结构以增强神经网络的学习能力。首先我们先来看看网络结构。由于网络结构变得越发复杂，从这一节开始我们将只是用表格来说明网络结构。</p>
<p>AlexNet的主要改进在于增大卷积核和卷积层的深度，并且使用了ReLU作为激活函数（原始的LeNet-5是使用sigmoid函数作为激活函数的，我们在上一节编写的LeNet-5已经做了改进，使用ReLU激活函数）。另外AlexNet还使用了很多图像增广的方法。图像增广是一种扩大数据集的使用方法，通过一些算法对原始的图像做旋转，裁剪等操作使神经网络可以学习到不同位置的同一个特征，让神经网络更加具有普适性。同样在识别图片的时候AlexNet也会翻转图像使神经网络多识别出一些特征，提高正确率。但由于AlexNet的参数数量大大提高，网络很容易过拟合。过拟合简单来说就是神经网络的参数数量已经多到可以“记住”每一张训练集的图片，所以在训练集上可以表现得非常出色，但在测试集上会惨不忍睹。为此，AlexNet引进了DropOut层。DropOut层会在训练时随机丢弃一些参数，使神经网络不会过度依赖一些参数“记住”图像。在测试时为了稳定，DropOut层会被关闭。也就是说使用了DropOut层的神经网络在训练和测试时网络是不一样的。</p>
<p>为了对比我们暂时先不进行图像增广处理，稍后再增加<code>transforms.CenterCrop</code>，<code>transforms.ColorJitter</code>，<code>transforms.FiveCrop</code>等图像增广函数看看训练效果如何。</p>
<p>AlexNet的结构如下。原始的AlexNet的输入尺寸是227*227，这里我们将改成128*128适应CINIC-10的大小，同时一些步长和填充参数也有调整。</p>
<table>
<thead>
<tr>
<th>层名</th>
<th>输入形状</th>
<th>输出形状</th>
<th>步长</th>
<th>填充</th>
<th>核/窗口尺寸</th>
</tr>
</thead>
<tbody>
<tr>
<td>conv1</td>
<td>3*128*128</td>
<td>96*40*40</td>
<td>3</td>
<td>0</td>
<td>11*11</td>
</tr>
<tr>
<td>pool1</td>
<td>96*40*40</td>
<td>96*20*20</td>
<td>2</td>
<td>0</td>
<td>2*2</td>
</tr>
<tr>
<td>conv2</td>
<td>96*20*20</td>
<td>256*20*20</td>
<td>1</td>
<td>2</td>
<td>5*5</td>
</tr>
<tr>
<td>pool2</td>
<td>256*20*20</td>
<td>256*10*10</td>
<td>2</td>
<td>0</td>
<td>2*2</td>
</tr>
<tr>
<td>conv3</td>
<td>256*10*10</td>
<td>384*10*10</td>
<td>1</td>
<td>1</td>
<td>3*3</td>
</tr>
<tr>
<td>conv4</td>
<td>384*10*10</td>
<td>384*10*10</td>
<td>1</td>
<td>1</td>
<td>3*3</td>
</tr>
<tr>
<td>conv5</td>
<td>384*10*10</td>
<td>256*10*10</td>
<td>1</td>
<td>1</td>
<td>3*3</td>
</tr>
<tr>
<td>pool3</td>
<td>256*10*10</td>
<td>256*5*5</td>
<td>2</td>
<td>0</td>
<td>2*2</td>
</tr>
<tr>
<td>linear1</td>
<td>6400</td>
<td>4096</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td>dropout1</td>
<td>4096</td>
<td>4096</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td>linear2</td>
<td>4096</td>
<td>4096</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td>dropout2</td>
<td>4096</td>
<td>4096</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td>linear3</td>
<td>4096</td>
<td>10</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
</tr>
</tbody>
</table>
<p>可以看到AlexNet的通道数和核大小相比LeNet来说都增加了不少。中间的卷积通过填充在不改变图像大小的情况下增加通道数，使神经网络可以学到更多特征。</p>
<h2 id="使用pytorch构建alexnet"><a class="markdownIt-Anchor" href="#使用pytorch构建alexnet"></a> 使用pytorch构建AlexNet</h2>
<p>同样我们构建一个类来表示这个网络</p>
<pre><code>from torch.nn.functional import relu
class AlexNet(torch.nn.Module):
    def __init__(self):   
        super(AlexNet, self).__init__()
        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=96, stride=3, kernel_size=11)
        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = torch.nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2)
        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv3 = torch.nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1)
        self.conv4 = torch.nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1)
        self.conv5 = torch.nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)
        self.pool3 = torch.nn.Conv2d(kernel_size=2, stride=2)
        self.linear1 = torch.nn.Linear(6400, 4096)
        #Dropout层需要有一个超参数p，指的是每个参数被丢弃的概率
        self.dropout1 = torch.nn.Dropout(0.5)
        self.linear2 = torch.nn.Linear(4096, 4096)
        self.dropout2 = torch.nn.Dropout(0.5)
        self.linear3 = torch.nn.Linear(4096, 10)      
        
    def forward(self, x):
        x = relu(self.conv1(x))  
        x = self.pool1(x)
        x = relu(self.conv2(x))
        x = self.pool2(x)
        x = relu(self.conv3(x))
        x = relu(self.conv4(x))
        x = relu(self.conv5(x))
        x = self.pool3(x)
        x = x.view(-1, 6400)
        x = relu(self.linear1(x))
        x = self.dropout1(x)
        x = relu(self.linear2(x))
        x = self.dropout2(x)
        x = self.linear3(x)        
        return x
    
net = AlexNet()
</code></pre>
<p>我们使用torchsummary和torchviz查看这个网络结构。</p>
<pre><code>from torchsummary import summary
#如果pytorch是否有CUDA支持需要将网络移动到显存中
net.to(torch.device(&quot;cuda&quot;))
summary(net, (3,128,128))
</code></pre>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/AlexNet-arch.png" srcset="undefined" class="">
<p>可以看到这个网络有接近四千七百万个参数，相比LeNet的四万多个参数已经大了一百倍。同样，计算图也要大上不少。</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/AlexNet_computational_graph.svg" srcset="undefined" class="">
<p>由于我们使用的是ReLU激活函数，初始化方法自然还是He初始化。</p>
<pre><code>def init_weights(m):
    if type(m) == torch.nn.Linear:
        torch.nn.init.kaiming_uniform_(m.weight)
        m.bias.data.fill_(0.01)
    if type(m) == torch.nn.Conv2d:
        torch.nn.init.kaiming_uniform_(m.weight)
        m.bias.data.fill_(0.01)
        
net.apply(init_weights)
criterion = torch.nn.CrossEntropyLoss()
</code></pre>
<p>最后加上优化函数就可以开始训练了。由于这个网络规模较大，建议使用GPU训练。</p>
<pre><code>net.to(torch.device(&quot;cuda&quot;))
optimizer = torch.optim.Adam(net.parameters(),lr=0.01)
for epoch in range(10):
    loss = 0
    for _, data in enumerate(train_loader):
        features, labels = data
        features, labels = features.cuda(), labels.cuda()
        optimizer.zero_grad()
        outputs = net(features)
        loss = criterion(outputs,labels)
        loss.backward()
        optimizer.step()

    #每运行完一代就显示训练效果
    print(&quot;epoch {}, loss {}&quot;.format(epoch+1,loss.data.item()))
</code></pre>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/AlexNet_0.01.png" srcset="undefined" class="">
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/AlexNet_0.01_accuracy.png" srcset="undefined" class="">
<p>奇怪和第一次我们使用的CNN结果一样，这次的损失丝毫没有下降，验证集的结果也是惨不忍睹。但我们已经使用了He初始化对权重进行初始化，那么还有什么地方需要调整呢？我们先来看一看和训练过程最相关的超参数：学习率。</p>
<h3 id="学习率指导模型学习进度"><a class="markdownIt-Anchor" href="#学习率指导模型学习进度"></a> 学习率指导模型学习进度</h3>
<p>反向传播需要多次更新权重以寻找最小值,而每次更新的幅度对学习成果有重要影响。控制每次更新幅度的参数就是优化算法的学习率。过大的学习率会导致网络错过最低点以至于不能收敛，而过小的学习率又会导致学习过于缓慢。下图可以很形象的表示这两种情况（<a href="https://www.educative.io/edpresso/learning-rate-in-machine-learning" target="_blank" rel="noopener">来源</a>）。</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/learningrate.png" srcset="undefined" class="">
<p>由此可见，在一开始我们应该使用较大的学习率，而在最终收敛阶段就需要使用较小的学习率防止网络错过最低点。不同的优化算法对学习率的处理各不相同，早期的算法使用固定的学习率进行更新，而新的算法则会动态降低学习率适应当前的学习情况。例如我们一直使用的Adam算法，全称即为ADAptive Moment estimation 适应性动量估计。事实上Adam算法有四个参数：学习率，beta1，beta2和epsilon用来控制学习率衰减的程度。算法的论文推荐这四个参数分别设置为0.001，0.9，0.999和1e-8，也就是pytorch的默认设置。因此我们也尝试使用这些参数进行训练。</p>
<p>将<br />
<code>optimizer = torch.optim.Adam(net.parameters(),lr=0.01)</code></p>
<p>改为<br />
<code>optimizer = torch.optim.Adam(net.parameters(),lr=0.001)</code></p>
<p>后，我们得到了更好的训练结果</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/AlexNet_0.001.png" srcset="undefined" class="">
<p>我们可以使用验证集确认一下训练的效果</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/AlexNet_0.001_accuracy.png" srcset="undefined" class="">
<p>可以看到准确率从随机乱猜的10%已经上升到了49%，效果极为显著。除了Adam算法以外，还有其他自适应学习率的算法，例如AdaDelta，以及使用动量控制学习进度的方法,例如SGD-nesterov，都可以根据学习进度调整更新的幅度。一般来说对于初学者Adam较为合适，而SGD-nesterov在精心调教下可以得到一个更好的结果，所以很多论文依然在用较老的SGD算法。这些优化算法都在pytorch中有官方实现，因此只需替换optimizer便可以体验不同的优化算法。</p>
<h3 id="使用图像增广扩大数据集"><a class="markdownIt-Anchor" href="#使用图像增广扩大数据集"></a> 使用图像增广扩大数据集</h3>
<p>在现实生活中，数据集的收集往往是比设计和训练神经网络更加困难的事。因此如何更有效地利用数据便成为了非常重要的研究对象。同时我们也要避免神经网络“记住”训练集的内容，增强它的泛化能力才能以不变应万变。这两个问题都可以通过图像增广改善。AlexNet的成功和它应用了多个图像增广方法密切相关。具体来说，AlexNet使用了三个图像增广方法：裁剪，翻转和随机颜色变化。这些图像增广的工具都包含在torchvision的transforms模块中。AlexNet使用这些方法将数据集扩大了2048倍，这足以显示图像增广可以大幅度提升已有数据的利用率。下面我们修改读取数据集的代码，应用一下这几个函数</p>
<pre><code>image_transforms = transforms.Compose(
    [transforms.Resize((148,148)), #将图片稍稍放大便于裁剪
    transforms.RandomHorizontalFlip(p=0.5), #RandomHorizontalFlip随机挑选图片水平翻转，被挑选的几率由p参数控制
    transforms.RandomResizedCrop(128), #RandomResizedCrop随机裁剪出图片上的一部分并缩放至需要的大小，缩放的大小由size参数控制，这里我们使用128x128。另外还有其他参数控制裁剪区域的选择与大小控制
    transforms.ColorJitter(brightness=0.5,contrast=0.5,saturation=0.5,hue=0.5), #ColorJitter随机调整亮度，对比度，饱和度和色调
    transforms.ToTensor(),
    transforms.Normalize(mean = [0.47889522, 0.47227842, 0.43047404],
                        std = [0.24205776, 0.23828046, 0.25874835])]
)
</code></pre>
<p>要注意只有训练集需要进行图像增广，而验证集和测试不需要，因此要分成两个函数处理。处理完毕后我们看一看处理的成果</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/augmentation.png" srcset="undefined" class="">
<p>可以看到相比一开始有些图片已经被很明显的处理过了。我们使用这个新的数据集进行训练。要注意的是pytorch并不会直接增大数据集大小，这些处理是在训练途中进行的。如果觉得由于图像增广训练缓慢可以试试<a href="https://github.com/albumentations-team/albumentations" target="_blank" rel="noopener">albumentations</a>这类库提前处理图像增广再训练。由于数据增广后难度增加，我们使用一个较小的学习率0.0001训练。</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/augmentation_train.png" srcset="undefined" class="">
<p>使用验证集和测试集检查训练结果，可以发现识别率又上升了12%左右，说明图像增广可以有效提升神经网络泛化的能力。</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/augmentation_valid.png" srcset="undefined" class="">
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/augmentation_test.png" srcset="undefined" class="">
<h2 id="使用caffe2构建alexnet"><a class="markdownIt-Anchor" href="#使用caffe2构建alexnet"></a> 使用caffe2构建AlexNet</h2>
<p>由于caffe2并没有提供CINIC-10的数据集，我们首先要自己构建数据集。这里我们参照caffe2的<a href="https://github.com/facebookarchive/tutorials/blob/master/CIFAR10_Part1.ipynb" target="_blank" rel="noopener">官方教程</a>构建。</p>
<p>在pytorch中，框架会自动将文件夹名作为标签导入，并为每一个标签赋一个数字索引。然而caffe2并不会自动完成这些工作，因此第一步就是要为每一个标签创建索引，并使用在caffe中很常用的标签文件将图片和分类索引挂钩。例如我们的索引是<code>{'airplane': 0,'automobile': 1,'bird': 2,'cat': 3,'deer': 4,'dog': 5,'frog': 6,'horse': 7,'ship': 8,'truck': 9}</code>，那标签文件就是</p>
<pre><code>CINIC-10/train/airplane/cifar10-train-29.png 0
CINIC-10/train/automobile/cifar10-train-4.png 1
CINIC-10/train/bird/cifar10-train-6.png 2
...
</code></pre>
<p>以此类推，为训练集，验证集和测试集分别创建标签文件。不同的数据集标签的方法不同，因此创建标签文件的代码自然也不同。这里我们根据CINIC-10的标签创建这一文件。</p>
<pre><code>import os
train_dataset_path = &quot;CINIC-10/train&quot;
test_dataset_path = &quot;CINIC-10/test&quot;
valid_dataset_path = &quot;CINIC-10/valid&quot;

labels = os.listdir(train_dataset_path) #CINIC-10的标签是每个子文件夹的名字，因此只需用os.listdir便可读取所有标签
classes = {label:index for index,label in enumerate(labels)} #使用数组下标作为索引
</code></pre>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/caffe2_classes.png" srcset="undefined" class="">
<p>准备好索引后我们就可以创建标签文件了</p>
<pre><code>train_label_file_path = &quot;CINIC-10/train-labels.txt&quot;
test_label_file_path = &quot;CINIC-10/test-labels.txt&quot;
valid_label_file_path = &quot;CINIC-10/valid-labels.txt&quot;
from random import shuffle
with open(train_label_file_path,'w') as f:
    train_files = []
    for root,dir,file in os.walk(train_dataset_path):
        root = root.replace(&quot;\\&quot;,&quot;/&quot;) #在windows中路径分隔符是\，为了统一全部换成/
        label = root.split(&quot;/&quot;)[-1] #路径的最后一个文件夹名即为标签名，因此将它提取出
        #使用一个列表生成式生成文件的每一行并用writelines填充文件
        train_files.extend([&quot;{root}/{filename} {clsindex}\n&quot;.format(root=root,filename=fn,clsindex=classes[label]) for fn in file])
    #打乱图像的顺序
    shuffle(train_files)
    f.writelines(train_files)

#以此类推生成验证集和测试集的标签文件
with open(test_label_file_path,'w') as f:
    test_files= []
    for root,dir,file in os.walk(test_dataset_path):
        root = root.replace(&quot;\\&quot;,&quot;/&quot;)
        label = root.split(&quot;/&quot;)[-1] 
        test_files.extend([&quot;{root}/{filename} {clsindex}\n&quot;.format(root=root,filename=fn,clsindex=classes[label]) for fn in file])
    shuffle(test_files)
    f.writelines(test_files)
        
with open(valid_label_file_path,'w') as f:
    valid_files = []
    for root,dir,file in os.walk(valid_dataset_path):
        root = root.replace(&quot;\\&quot;,&quot;/&quot;)
        label = root.split(&quot;/&quot;)[-1] 
        valid_files.extend([&quot;{root}/{filename} {clsindex}\n&quot;.format(root=root,filename=fn,clsindex=classes[label]) for fn in file])
    shuffle(valid_files)
    f.writelines(valid_files)
</code></pre>
<p>接下来打开训练用标签文件检查是否正确生成</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/label_files.png" srcset="undefined" class="">
<h3 id="使用albumentations进行图像增广"><a class="markdownIt-Anchor" href="#使用albumentations进行图像增广"></a> 使用albumentations进行图像增广</h3>
<p>在写入数据库前，首先我们需要考虑图像增广。由于caffe2的图像增广功能依赖OpenCV，而OpenCV编译也需要大量时间，因此在安装pytorch时我们默认没有安装OpenCV支持。因此我们使用之前介绍的图像增广库<a href="https://github.com/albumentations-team/albumentations" target="_blank" rel="noopener">albumentations</a>先生成增广后的图片再制作数据集。如果已经安装OpenCV支持可以跳过这一段，直接构建数据集，在之后导入时我会同时演示这两种方法对应的导入代码。</p>
<p>albumentations是一个功能齐全的图像增广库，可以对图像进行变色，翻转，扭曲，遮罩等多种处理。这里我们只使用和pytorch相同的翻转，裁剪和变色三种方法。首先安装albumentation：</p>
<pre><code>conda install -c conda-forge imgaug
conda install albumentations -c conda-forge
</code></pre>
<p>albumentations有着和pytorch相似的API，因此很容易上手。我们先用一张图片尝试尝试</p>
<pre><code>import albumentations as A
from imageio import imread
import matplotlib.pyplot as plt
%matplotlib inline
mean = (0.47889522, 0.47227842, 0.43047404)
std = (0.24205776, 0.23828046, 0.25874835)
img = imread(&quot;CINIC-10/train/airplane/cifar10-train-29.png&quot;)

aug = A.Compose([
    A.Resize(width=148,height=148),
    A.HorizontalFlip (p=0.5),
    A.RandomResizedCrop(width=128,height=128),
    A.ColorJitter(brightness=0.5,contrast=0.5,saturation=0.5,hue=0.5),
    A.Normalize(mean=mean,std=std)
],p=1)
aug_img = aug(image=img)
figs,axs = plt.subplots(1,2, figsize=(20,20)) 

aug_img['image'] *= std
aug_img['image'] += mean
axs[0].imshow(img)
axs[1].imshow(aug_img['image'])

{% asset_img albumentation_test.png %}
</code></pre>
<p>可以看到图片已经经过了一定的处理。接下来我们将这个函数融入下面的<code>write_lmdb</code>中创建数据集。</p>
<p>标签文件制作完成后我们就可以根据这个文件制作lmdb或leveldb数据库供caffe2使用了，下面这个函数改编自官方教程的write_lmdb函数。除了能够写入caffe2的ImageInput能直接读取的图像格式以外，我们还增加了和图像增广有关的函数。为了方便我们直接将原始的数据集和增广后的数据集一起写入数据库中。</p>
<pre><code>import lmdb
import numpy as np
from caffe2.proto import caffe2_pb2
from imageio import imread

def write_lmdb(labels_file_path, lmdb_path, augmentation=False,is_test=False,mean=[],std=[]):
    with open(labels_file_path, 'r') as labels_handler:
        print(&quot;&gt;&gt;&gt; Write database...&quot;)
        if augmentation is False:
            LMDB_MAP_SIZE = 1 &lt;&lt; 31 # map size代表LMDB能够达到最大的大小，默认为1&lt;&lt;40，即1TB。在某些系统上这会导致系统直接分配1TB空间而不是使用稀疏文件表示。
                                # 因此我们设置为一个贴近数据集的大小1&lt;&lt;31，即2GB
        else:
            LMDB_MAP_SIZE = 1 &lt;&lt; 35 #使用albumentations进行数据增广时数据集大小会变成17GB，因此将大小设置为1&lt;&lt;35，即32GB
        env = lmdb.open(lmdb_path, map_size=LMDB_MAP_SIZE)

        with env.begin(write=True) as txn:
            count = 0
            for line in labels_handler.readlines(): # 从标签文件中读取每一行
                line = line.rstrip()
                im_path = line.split()[0] # 标签文件第一部分是文件名，第二部分是分类索引
                im_label = int(line.split()[1])
                if augmentation is True:
                    from imageio import imread
                    img_data = imread(im_path).astype(np.float32) # 使用imageio库读取图片并转换为numpy数组，默认读取的格式是HWC，即(高,宽,通道)
                else:                                             # 彩色图片有三个通道，即RGB
                    from cv2 import imread,IMREAD_UNCHANGED #如果使用ImageInput则使用cv2读取数据，OpenCV默认读取为BGR格式，和caffe2一致
                    img_data = imread(im_path,IMREAD_UNCHANGED) # caffe2的ImageInput接受原始图像输入，因此使用cv2的imread和tobytes转换为字节字符串。IMREAD_UNCHANGED阻止OpenCV将灰度图转成彩色图
                    im_bytes = img_data.tobytes()
                if len(img_data.shape) == 3: #在CINIC-10中有大约100多张灰度图（没有通道维），为了方便处理我们只使用彩色图  
                    if augmentation is True:
                        normalization = A.Compose([
                            A.Normalize(mean=mean,std=std),
                            A.Resize(width=128,height=128)],p=1) #如果使用图像增广则原始图像使用albumentation归一化
                        img_data = normalization(image=img_data)['image']
                        img_data = img_data[:, :, (2, 1, 0)] # caffe2沿用caffe的BGR格式，因此要将RGB三个通道调换位置，这是由于caffe使用OpenCV处理图像
                                                            # 而OpenCV使用BGR表示图像
                        img_data = np.transpose(img_data, (2,0,1)) # 将HWC转换为CHW，即(通道,高,宽)，这么做是为了能够使用GPU加速。Nvidia的cuDNN库只支持CHW格式
                                                                # 之前MNIST数据集使用NCHW也是源于此，N为每一个小批量的大小
                    # 将图像存储在caffe2张量中，每个tensor_protos存储两个张量，即图像本身和标签
                    tensor_protos = caffe2_pb2.TensorProtos()
                    img_tensor = tensor_protos.protos.add() 
                    img_tensor.dims.extend(img_data.shape) 
                    if augmentation is True:
                        img_tensor.data_type = 1 # data_type对应caffe2的DataType枚举，1代表float
                        flatten_img = img_data.reshape(np.prod(img_data.shape))
                        img_tensor.float_data.extend(flatten_img)
                    else:
                        img_tensor.data_type = 3 #如果使用caffe2自带的ImageInput处理图像则将data_type设为3，即byte
                        img_tensor.byte_data = im_bytes
                    label_tensor = tensor_protos.protos.add() 
                    label_tensor.data_type = 2 #2代表int32
                    label_tensor.int32_data.append(im_label)
                    txn.put(
                        '{}'.format(count).encode('ascii'), #lmdb是键值数据库，键为当前处理图像的编号
                        tensor_protos.SerializeToString() #值即为两个张量
                    )
                    count = count + 1
                    # 执行图像增广并制作张量 如果是在制作测试数据集则跳过增广
                    if augmentation is True and is_test is False:
                        img_data = imread(im_path).astype(np.float32)
                        aug = A.Compose([
                            A.Resize(width=148,height=148),
                            A.HorizontalFlip (p=0.5),
                            A.RandomResizedCrop(width=128,height=128),
                            A.ColorJitter(brightness=0.5,contrast=0.5,saturation=0.5,hue=0.5),
                            A.Normalize(mean=mean,std=std)
                        ],p=1)
                        aug_img_data = aug(image=img_data)['image']
                        aug_img_data = aug_img_data[:, :, (2, 1 ,0)]
                        aug_img_data = np.transpose(aug_img_data,(2,0,1))
                        aug_tensor_protos = caffe2_pb2.TensorProtos()
                        aug_img_tensor = aug_tensor_protos.protos.add() 
                        aug_img_tensor.dims.extend(aug_img_data.shape) 
                        aug_img_tensor.data_type = 1
                        aug_flatten_img = aug_img_data.reshape(np.prod(aug_img_data.shape))
                        aug_img_tensor.float_data.extend(aug_flatten_img)
                        aug_label_tensor = aug_tensor_protos.protos.add() 
                        aug_label_tensor.data_type = 2
                        aug_label_tensor.int32_data.append(im_label)
                        count = count + 1
                        txn.put(
                            '{}'.format(count).encode('ascii'), 
                            aug_tensor_protos.SerializeToString()
                        )
                    if ((count % 1000 == 0)):
                        print(&quot;Inserted {} rows&quot;.format(count))

    print(&quot;Inserted {} rows&quot;.format(count))
    print(&quot;\nLMDB saved at &quot; + lmdb_path + &quot;\n\n&quot;)
</code></pre>
<p>要注意这里data_type属性和caffe2的DataType对应，具体的关系是</p>
<pre><code>enum DataType {
 UNDEFINED = 0;
 FLOAT = 1;  // float
 INT32 = 2;  // int
 BYTE = 3;  // BYTE, when deserialized, is going to be restored as uint8.
 STRING = 4;  // string
 BOOL = 5;  // bool
 UINT8 = 6;  // uint8_t
 INT8 = 7;  // int8_t
 UINT16 = 8;  // uint16_t
 INT16 = 9;  // int16_t
 INT64 = 10;  // int64_t
 FLOAT16 = 12;  // at::Half
 DOUBLE = 13;  // double
}
</code></pre>
<p>如果需要不同的数据类型可以通过修改这个值实现。LevelDB除了用于读写的库不同其他都相同，因此不再赘述。接下来便可直接使用该函数创建数据库</p>
<pre><code>train_lmdb_path = &quot;CINIC-10/train.db&quot;
test_lmdb_path = &quot;CINIC-10/test.db&quot;
valid_lmdb_path = &quot;CINIC-10/valid.db&quot;
mean = (0.47889522, 0.47227842, 0.43047404)
std = (0.24205776, 0.23828046, 0.25874835)
# 如需要使用albumentation进行图像增广
write_lmdb(train_label_file_path,train_lmdb_path,augmentation=True,mean=mean,std=std)
write_lmdb(test_label_file_path,test_lmdb_path,augmentation=True,is_test=True,mean=mean,std=std)
write_lmdb(valid_label_file_path,valid_lmdb_path,augmentation=True,is_test=True,mean=mean,std=std)
# 如需要使用caffe2自带的图像增广功能
write_lmdb(train_label_file_path,train_lmdb_path)
write_lmdb(test_label_file_path,test_lmdb_path)
write_lmdb(valid_label_file_path,valid_lmdb_path)
</code></pre>
<p>使用训练集测试函数是否正常运作，这里不使用图像增广</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/lmdb_train.png" srcset="undefined" class="">
<p>可以看到成功向数据库中写了89895张图片，由于我们剔除了灰度图，训练集比CINIC-10原本包含的90000张图少了一些，但对训练没有什么影响。同样我们可以制作出验证集和测试集的数据库。如果使用图像增广</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/albumentation_train_set.png" srcset="undefined" class="">
<p>可以看到写入的数据量多了一倍，也就是说数据集现在同时包含原始图像和增广后的图像</p>
<p>数据库制作好后，我们就可以开始构建网络了。如果之前使用albumentation制作图像增广，载入数据库的代码与之前基本一致。由于我们在写入数据库时已经做了归一化，原本用于简易归一化的Scale操作符和转换数据类型的Cast操作符也不需要了。</p>
<pre><code>with core.DeviceScope(core.DeviceOption(caffe2_pb2.CUDA, 0)): #由于CINIC-10数据集较大，我们使用CUDA训练，如没有支持CUDA的显卡则删去这行
    from caffe2.python import brew, core, model_helper, optimizer, workspace, net_drawer
    train_model = model_helper.ModelHelper(&quot;train_model&quot;,arg_scope={&quot;order&quot;:&quot;NCHW&quot;})
    features, labels = train_model.TensorProtosDBInput([],[&quot;features&quot;,&quot;labels&quot;],batch_size=50,db=&quot;CINIC-10/train_aug.db&quot;,db_type=&quot;lmdb&quot;)
    features = train_model.StopGradient(features,features)
</code></pre>
<p>如果在编译pytorch时已经编译了OpenCV2支持，可以使用一个新的操作符<a href="https://caffe2.ai/docs/operators-catalogue.html#imageinput" target="_blank" rel="noopener">ImageInput</a>取代直接读取张量的TensorProtosDBInput。ImageInput自带图像增广方法，只需将参数传入即可调用。同样删去用于归一化的<code>features = train_model.Scale(features,features,scale=float(1/256))</code>。brew也为ImageInput提供了一个简单的包装，这里使用brew.image_input调用该操作符。</p>
<pre><code>with core.DeviceScope(core.DeviceOption(caffe2_pb2.CUDA, 0)):
    from caffe2.python import brew, core, model_helper, optimizer, workspace, net_drawer
    train_model = model_helper.ModelHelper(&quot;train_model&quot;,arg_scope={&quot;order&quot;:&quot;NCHW&quot;})
    features, labels = brew.image_input(train_model,train_reader,[&quot;features&quot;,&quot;labels&quot;],
                                                        color=3,color_jitter=1,
                                                        mirror=1,scale=224,
                                                        crop=128,
                                                        mean_per_channel=tuple((x*256 for x in (0.43047404,0.47227842,0.47889522))),#ImageInput的平均值和标准差取值范围是0-256而不是0-1，因此需要转换一下
                                                        std_per_channel=tuple((x*256 for x in (0.25874835,0.23828046,0.24205776))),
                                                        use_caffe_datum=False,use_gpu_transform=1, #如果有CUDA则设置use_gpu_transform=1，ImageInput会自动将图像转为NCHW。
                                                        is_test=False,batch_size=50)
    features = train_model.net.NHWC2NCHW(features,&quot;features_nchw&quot;) #如没有CUDA设备则调用NHWC2NCHW操作符转换
    features = train_model.StopGradient(features,features)
</code></pre>
<p>接下来只需将AlexNet在caffe2中写一遍</p>
<pre><code>with core.DeviceScope(core.DeviceOption(caffe2_pb2.CUDA, 0)):
    conv1 = brew.conv(train_model, features, &quot;conv1&quot;,dim_in=3,dim_out=96,weight_init=(&quot;MSRAFill&quot;,{}),kernel=11,stride=3)
    relu1 = brew.relu(train_model, conv1, &quot;relu1&quot;)
    pool1 = brew.max_pool(train_model, relu1, &quot;pool1&quot;,kernel=2,stride=2)
    conv2 = brew.conv(train_model, pool1, &quot;conv2&quot;, dim_in=96,dim_out=256, weight_init=(&quot;MSRAFill&quot;,{}),kernel=5,pad=2)
    relu2 = brew.relu(train_model,conv2,&quot;relu2&quot;)
    pool2 = brew.max_pool(train_model,relu2,&quot;pool2&quot;,kernel=2,stride=2)
    conv3 = brew.conv(train_model, pool2, &quot;conv3&quot;, dim_in=256,dim_out=384, weight_init=(&quot;MSRAFill&quot;,{}),kernel=3,pad=1)
    relu3 = brew.relu(train_model,conv3,&quot;relu3&quot;)
    conv4 = brew.conv(train_model, relu3, &quot;conv4&quot;, dim_in=384,dim_out=384, weight_init=(&quot;MSRAFill&quot;,{}),kernel=3,pad=1)
    relu4 = brew.relu(train_model,conv4,&quot;relu4&quot;)
    conv5 = brew.conv(train_model, relu4, &quot;conv5&quot;, dim_in=384,dim_out=256, weight_init=(&quot;MSRAFill&quot;,{}),kernel=3,pad=1)
    relu5 = brew.relu(train_model,conv5,&quot;relu5&quot;)
    pool3 = brew.max_pool(train_model,relu5,&quot;pool3&quot;,kernel=2,stride=2)
    fc1 = brew.fc(train_model,pool3,&quot;fc1&quot;,dim_in=256*5*5,dim_out=4096,weight_init=(&quot;MSRAFill&quot;,{}))
    relu6 = brew.relu(train_model,fc1,&quot;relu6&quot;)
    dropout1 = brew.dropout(train_model, relu6, &quot;dropout1&quot;, ratio=0.5，is_test=False)
    fc2 = brew.fc(train_model,dropout1,&quot;fc2&quot;,dim_in=4096,dim_out=4096,weight_init=(&quot;MSRAFill&quot;,{}))
    relu7 = brew.relu(train_model,fc2,&quot;relu7&quot;)
    dropout2 = brew.dropout(train_model,relu7,&quot;dropout2&quot;,ratio=0.5,is_test=False)
    fc3 = brew.fc(train_model,dropout2,&quot;fc3&quot;,dim_in=4096,dim_out=10,weight_init=(&quot;MSRAFill&quot;,{}))
    softmax = brew.softmax(train_model,fc3,&quot;softmax&quot;)
</code></pre>
<p>加入交叉熵函数</p>
<pre><code>with core.DeviceScope(core.DeviceOption(caffe2_pb2.CUDA, 0)):
    crossEntropy = train_model.LabelCrossEntropy([softmax,labels],&quot;crossEntropy&quot;)
    loss = train_model.AveragedLoss(crossEntropy, &quot;loss&quot;)
</code></pre>
<p>确认计算图</p>
<pre><code>from IPython import display
graph = net_drawer.GetPydotGraph(train_model.Proto().op,&quot;train&quot;,rankdir=&quot;LR&quot;)
display.Image(graph.create_png(),width=800)
</code></pre>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/alexmap_graph.png" srcset="undefined" class="">
<p>加入优化算法，学习率设为0.0001</p>
<pre><code>with core.DeviceScope(core.DeviceOption(caffe2_pb2.CUDA, 0)):
    train_model.AddGradientOperators([loss])
    optimizer.build_adam(
        train_model,
        base_learning_rate=0.0001,
        policy=&quot;step&quot;,
        stepsize=1,
        gamma=0.999,
    )
</code></pre>
<p>每个小批量大小为50，我们需要迭代10代。如使用albumentation增广，训练集大小为179790，即循环35958次。如果使用caffe2的ImageInput在运行时处理图像训练集大小不变，即89895张图片，循环17979次。我们这里使用ImageInput处理图像，因此设置循环17979次。</p>
<pre><code>workspace.RunNetOnce(train_model.param_init_net)
workspace.CreateNet(train_model.net)
total_iters = 17979
loss = [0 for i in range(17979)]

for i in range(total_iters):
	workspace.RunNet(train_model.net.Proto().name)
	loss[i] = workspace.FetchBlob('loss')
    if i%1000 == 0:
        print(&quot;Current loss: {}&quot;.format(loss))
print(loss)
</code></pre>
<p>使用Albumentations的loss如下</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/albumentations_loss.png" srcset="undefined" class="">
<p>使用ImageInput的loss如下</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/ImageInput_loss.png" srcset="undefined" class="">
<p>接下来使用训练集和测试集检验结果。为了方便我将验证集用于ImageInput，测试集用于Albumentations。</p>
<pre><code>with core.DeviceScope(core.DeviceOption(caffe2_pb2.CUDA,0)):
    test_model = model_helper.ModelHelper(&quot;test_model&quot;,arg_scope={&quot;order&quot;:&quot;NCHW&quot;},init_params=False)
    #使用ImageInput
    valid_reader = test_model.CreateDB(&quot;valid_reader&quot;,db_type=&quot;lmdb&quot;,db=&quot;CINIC-10/valid.db&quot;,num_shards=1,shard_id=0)
    features, labels = brew.image_input(test_model,valid_reader,[&quot;features&quot;,&quot;labels&quot;],
                                                        color=3,color_jitter=0, #关闭图像增广参数
                                                        mirror=0,scale=128,
                                                        crop=128,
                                                        mean_per_channel=tuple((x*256 for x in (0.43047404,0.47227842,0.47889522))),
                                                        std_per_channel=tuple((x*256 for x in (0.25874835,0.23828046,0.24205776))),
                                                        use_caffe_datum=False,use_gpu_transform=1, 
                                                        is_test=True,batch_size=200) #设置is_test启用测试模式
    #使用Albumentations
    #features, labels = test_model.TensorProtosDBInput([],[&quot;features&quot;,&quot;labels&quot;],batch_size=200,db=&quot;CINIC-10/test.db&quot;,db_type=&quot;lmdb&quot;)
    features = test_model.StopGradient(features,features)
    conv1 = brew.conv(test_model, features, &quot;conv1&quot;,dim_in=3,dim_out=96,kernel=11,stride=3)
    relu1 = brew.relu(test_model, conv1, &quot;relu1&quot;)
    pool1 = brew.max_pool(test_model, relu1, &quot;pool1&quot;,kernel=2,stride=2)
    conv2 = brew.conv(test_model, pool1, &quot;conv2&quot;, dim_in=96,dim_out=256,kernel=5,pad=2)
    relu2 = brew.relu(test_model,conv2,&quot;relu2&quot;)
    pool2 = brew.max_pool(test_model,relu2,&quot;pool2&quot;,kernel=2,stride=2)
    conv3 = brew.conv(test_model, pool2, &quot;conv3&quot;, dim_in=256,dim_out=384,kernel=3,pad=1)
    relu3 = brew.relu(test_model,conv3,&quot;relu3&quot;)
    conv4 = brew.conv(test_model, relu3, &quot;conv4&quot;, dim_in=384,dim_out=384,kernel=3,pad=1)
    relu4 = brew.relu(test_model,conv4,&quot;relu4&quot;)
    conv5 = brew.conv(test_model, relu4, &quot;conv5&quot;, dim_in=384,dim_out=256,kernel=3,pad=1)
    relu5 = brew.relu(test_model,conv5,&quot;relu5&quot;)
    pool3 = brew.max_pool(test_model,relu5,&quot;pool3&quot;,kernel=2,stride=2)
    fc1 = brew.fc(test_model,pool3,&quot;fc1&quot;,dim_in=256*5*5,dim_out=4096)
    relu6 = brew.relu(test_model,fc1,&quot;relu6&quot;)
    dropout1 = brew.dropout(test_model, relu6, &quot;dropout1&quot;, ratio=0.5,is_test=True)
    fc2 = brew.fc(test_model,dropout1,&quot;fc2&quot;,dim_in=4096,dim_out=4096)
    relu7 = brew.relu(test_model,fc2,&quot;relu7&quot;)
    dropout2 = brew.dropout(test_model,relu7,&quot;dropout2&quot;,ratio=0.5,is_test=True)
    fc3 = brew.fc(test_model,dropout2,&quot;fc3&quot;,dim_in=4096,dim_out=10)
    softmax = brew.softmax(test_model,fc3,&quot;softmax&quot;)
    accuracy = brew.accuracy(test_model, [softmax,labels],&quot;accuracy&quot;)
</code></pre>
<p>Albumentations的结果如下</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/albumentations_accuracy.png" srcset="undefined" class="">
<p>ImageInput的结果如下</p>
<img src="/2020/05/02/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E4%BD%BF%E7%94%A8CINIC-10%E8%AE%AD%E7%BB%83AlexNet/ImageInput_accuracy.png" srcset="undefined" class="">
<p>至此我们迈出了深度卷积神经网络的第一步————AlexNet。下面我们还会继续认识另外两个对卷积神经网络的改进网络结构：NiN和ResNet。</p>
<hr class="footnotes-sep" />
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Normalization这个词可以翻译成正规化，标准化，归一化等，是一个较大的概念，指的是几种特征缩放(Feature Scaling)的方法。包括min-max归一化(min-max normalization)，均值归一化(mean normalization)和标准化(standarization)。 <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">神经网络</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/pytorch">pytorch</a>
                
                  <a class="hover-with-bg" href="/tags/caffe2">caffe2</a>
                
                  <a class="hover-with-bg" href="/tags/CNN">CNN</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
      <br><br>
      
      
  <div id="vcomments" style="width: 90%; margin: 0 auto;"></div>
  <script defer src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script defer src="//unpkg.com/valine/dist/Valine.min.js"></script>

  <script>
    var notify = 'false' === true;
    var verify = 'true' === true;
    var oldLoad = window.onload;
    window.onload = function () {
      new Valine({
        el: '#vcomments',
        notify: notify,
        verify: verify,
        app_id: "jY5stlz5eNHmsmr1p2wsi53q-MdYXbMMI",
        app_key: "R0kCXXt8GsmjHF9Nug1fSRWp",
        placeholder: "说点什么",
        avatar: "retro",
        meta: ['nick', 'mail', 'link'],
        pageSize: "10",
      });
      oldLoad && oldLoad();
    };
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://valine.js.org" target="_blank" rel="nofollow noopener noopener">comments
      powered by Valine.</a></noscript>


    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>


  <script src="/js/lazyload.js" ></script>



  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "pytorch&caffe2~深度学习的魔法~使用CINIC-10训练更多网络&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>



  

  
    <!-- KaTeX -->
    <link rel="stylesheet" href="/lib/katex/katex.min.css"  >
  





</body>
</html>
