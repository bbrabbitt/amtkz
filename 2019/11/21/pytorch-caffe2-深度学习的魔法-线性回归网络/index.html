<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="">
  <meta name="author" content="Amatsukazedaze">
  <meta name="keywords" content="">
  <title>pytorch&amp;caffe2 深度学习的魔法 线性回归网络 ~ hanayuki</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.1"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>hanayuki</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">Archives</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">Categories</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">Tags</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">About</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('/img/hanayuki.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期四, 十一月 21日 2019, 10:16 晚上
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    5.3k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      19 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <p>从这节开始，我们就要正式接触神经网络了。在构造更加复杂的神经网络之前，我们从一个简单的例子开始——线性回归。只要初学过统计甚至使用过一些电子表格软件（如Excel）的人都知道，线性回归可以让计算机在数据点间找到一个规律，并通过一个线性函数表示出来。虽然简单，但这个神经网络已经包含了神经网络必须的要素。在构造网络前，我们先来分析线性回归的模型是什么样的。</p>
<h1 id="线性回归模型"><a class="markdownIt-Anchor" href="#线性回归模型"></a> 线性回归模型</h1>
<p>在最简单的线性回归中，我们需要找的是如下函数的参数</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>y</mi><mo stretchy="true">^</mo></mover><mo>=</mo><msub><mi>w</mi><mn>1</mn></msub><mi>x</mi><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\widehat{y}=w_1x+b_1
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.865em;vertical-align:-0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.67056em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span><span class="svg-align" style="width:calc(100% - 0.11112em);margin-left:0.11112em;top:-3.43056em;"><span class="pstrut" style="height:3em;"></span><span style="height:0.24em;"><svg width='100%' height='0.24em' viewBox='0 0 1062 239' preserveAspectRatio='none'><path d='M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">w_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>被称作权重(weight)，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">b_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>被称作偏差(bias)，这两个值是神经网络的参数。另外两个变量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>是这个模型的输入，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>y</mi><mo stretchy="true">^</mo></mover></mrow><annotation encoding="application/x-tex">\widehat{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.865em;vertical-align:-0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.67056em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span><span class="svg-align" style="width:calc(100% - 0.11112em);margin-left:0.11112em;top:-3.43056em;"><span class="pstrut" style="height:3em;"></span><span style="height:0.24em;"><svg width='100%' height='0.24em' viewBox='0 0 1062 239' preserveAspectRatio='none'><path d='M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span></span></span>是这个模型的输出，即线性回归的预测值。为了能让网络得到正确的预测值，我们需要训练神经网络，即使用一系列已知的输入和输出的关系让神经网络自行找出正确的参数。这个过程通过神经网络反向传播完成。下面我们先简单熟悉一下什么是反向传播。具体的数学定义可以查看<a href="https://en.wikipedia.org/wiki/Backpropagation" target="_blank" rel="noopener">wikipedia</a>介绍和经典的<a href="https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf" target="_blank" rel="noopener">Learning representations by back-propagating errors（通过反向传播误差学习）</a>论文了解。</p>
<h1 id="反向传播简介"><a class="markdownIt-Anchor" href="#反向传播简介"></a> 反向传播简介</h1>
<p>反向传播是结合误差分析和微分使神经网络自行学习的方法。我们在这里不会接触任何数学知识，只需有一个最粗浅的认识。对于任何一个神经网络来说，数据在其中可以向两个方向流动：正向和反向。正向即神经网络的推理过程：给出一个输入，数据流经神经网络的整个计算图，然后得出一个输出。例如下面这个神经网络</p>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/FCNN.png" srcset="undefined" class="">
<p>一共有四层。最左侧的叫输入层，最右侧的叫输出层，而中间的叫隐藏层。我们的线性回归网络较这个网络简单的多，只有两层，也就是输入层和输出层。输入层上没有实际的运算，因此我们的网络实际只有一层，所以叫单层神经网络。如下图</p>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/simplest_net.png" srcset="undefined" class="">
<p>是上文中线性回归网络的计算图。图中的圆圈被称作节点或神经元，节点中间的线被称作边。在计算时神经网络会将输入与边上的权重<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">w_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>相乘，然后再与节点上的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">b_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>相加，最后得出一个预测的结果。</p>
<p>反向传播则更加难懂，它是神经网络的训练过程。简单来说，我们给出一组输入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>5</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>9</mn></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">X=\left(\begin{matrix}
  1 \\
  5 \\
  9
\end{matrix}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.60004em;vertical-align:-1.55002em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎝</span></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎛</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">5</span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">9</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎠</span></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span></span></span></span></span>和一组已知的输出<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>5</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>17</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>29</mn></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">y=\left(\begin{matrix}
  5 \\
  17 \\
  29
  \end{matrix}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.60004em;vertical-align:-1.55002em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎝</span></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎛</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">5</span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord">7</span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord">9</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎠</span></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span></span></span></span></span>（注意神经网络使用的数据均以张量形式表示），神经网络会使用随机的权重和偏差给出一个预测值（通常是根据神经网络启动时的随机参数产生的），用一个损失函数(loss function)得出网络的预测值和我们提供的真实值差距有多大，然后通过一些微分步骤修正每一个节点和每一条边上的参数，重复这个过程即可得到一个比较合理的参数。当神经网络能够得到一个合理的参数，我们就称网络收敛(converge)。我们给出的这组已知的输入和输出的集合被称作训练集，训练集的输入被称作特征(features)，而输出被称作标签(labels)。</p>
<p>反向传播的核心在于通过微分修正参数，这个过程可以想象成一个小球在山坡上滚动。它的任务是找到这个山的最低点（即神经网络能够收敛的点）。因为这个球很小，它看不见山的全貌，但是它知道哪里地势高一些，哪里地势低一些。于是它每次就向地势低的地方滚动一小段距离，然后再看看周围哪里地势更低，如此往复就有较大可能找到这个山的最低点。神经网络中这座山的陡峭程度被称作梯度，而小球沿着地势滚动的过程即自动求梯度的过程。神经网络通过自动求梯度可以找到一条收敛可能性较大的道路。这个过程称作梯度下降(Gradient Descent)。</p>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/gradient_descent.png" srcset="undefined" class="">
<center>梯度下降示意图 <a href="https://www.youtube.com/watch?v=5u0jaA3qAGk&feature=youtu.be" target="_blank" rel="noopener">来源</a></center>
<h1 id="使用pytorch构建网络"><a class="markdownIt-Anchor" href="#使用pytorch构建网络"></a> 使用pytorch构建网络</h1>
<h2 id="建立训练集"><a class="markdownIt-Anchor" href="#建立训练集"></a> 建立训练集</h2>
<p>在了解了神经网络的一些基本原理之后，我们可以使用pytorch实际训练一个线性回归网络了。首先我们需要准备的是训练集。通常这是神经网络中最难准备的部分（神经网络使用大量数据才能够应付各种情况）。幸运的是，线性回归的数据集准备很简单，只需要使用numpy生成一些随机数就可以了。</p>
<p>使用<code>conda activate pytorch</code>激活pytorch环境，在一个你喜欢的目录运行<code>jupyter notebook</code>，jupyter应该会自动打开浏览器,如果没有自动打开，在命令行中也会显示一个网址用来复制粘贴。</p>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/jupyter.png" srcset="undefined" class="">
<p>点击右上角新建一个Python3笔记本，然后运行</p>
<pre><code>#matplotlib是一个python的绘图库，我们用这个库查看数据的分布情况
#%matplotlib inline可以让matplotlib画出的图直接在jupyter中显示
%matplotlib inline
from IPython import display
from matplotlib import pyplot as plt
import torch
from torch.distributions.normal import Normal
true_w = 3
true_b = 2 #我们使用的线性函数的真实权重与偏差
           #即y=3x+2
#torch.randn函数使用一个标准正态分布生成随机数（期望值为0，标准差为1）
#size定义了生成的张量形状，500表示有500个样本
#1表示每个样本有一个输入         
features = torch.randn(size=(500,1))
#为了增加干扰，加入一些噪声
#噪声使用一个标准差为0.1的正态分布生成
#torch.distributions.normal.Normal类可以让我们自定义期望值和标准差
noise = Normal(loc=0,scale=0.1)
#pytorch的张量可以直接与数字（标量）进行运算，效果与广播类似
#这一步通过真实的权重与偏差算出特征对应的标签
#噪声是一个Normal类的实例，sample方法接受一个size参数，返回一个随机数张量
labels = true_w * features + true_b + noise.sample((500,1))
</code></pre>
<p>这样训练集就已经制作完成了，我们可以使用for循环查看输入和输出的对应关系，或者使用matplotlib更加直观的画出线性关系</p>
<pre><code>#用for循环打印前十个特征和标签
for X,y in zip(features[:9],labels[:9]):
print(X,y)
#用matplotlib画出图像
#使用svg画出更加清晰的图
display.set_matplotlib_formats('svg')
#matplotlib只接受numpy数组
#pytorch张量的numpy方法可以将pytorch张量转为numpy数组
plt.scatter(features.numpy(), labels.numpy(), 1);
</code></pre>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/training_data.png" srcset="undefined" class="">
<p>可以看到数据点间的有着较强的线性关系。</p>
<h2 id="准备数据"><a class="markdownIt-Anchor" href="#准备数据"></a> 准备数据</h2>
<p>出于性能考虑，神经网络通常不会直接使用500个数据训练，而是每次使用较少的样本（如10个），每个小样本被称作小批量。神经网络会随机从所有数据中取出10个数据，根据这10个数据计算梯度，然后沿着梯度方向下降至最小点。pytorch提供了TensorDataset和DataLoader两个类将刚刚生成的数据转成pytorch使用的数据集。</p>
<pre><code>from torch.utils.data import TensorDataset, DataLoader
#TensorDataset可以将两个第一个维度大小相同的张量合并成一个数据集
train_dataset = TensorDataset(features,labels)
#DataLoader接受上一步生成的数据集，生成小批量，可以自定义批量大小和数据用完后是否重新排序
dataloader = DataLoader(train_dataset,batch_size=10,shuffle=True)
</code></pre>
<p>可以用for循环提取出所有的特征和标签的关系</p>
<pre><code>for data in dataloader:
	print(data)
</code></pre>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/datasets.png" srcset="undefined" class="">
<h2 id="构建网络"><a class="markdownIt-Anchor" href="#构建网络"></a> 构建网络</h2>
<p>在做好所有的准备工作后，我们终于可以构建这个线性回归网络了。之前讲过神经网络的结构是用计算图表示的，其中会包括一些计算层。pytorch已经预先定义了很多神经网络常用的层，我们只需要调用这些函数就行了。大部分层的定义都在pytorch的nn模块，nn即neural network的缩写，我们先导入这个模块并创建一个网络。</p>
<pre><code>from torch import nn
net = nn.Sequential(nn.Linear(1,1))
</code></pre>
<p>这就是所有需要做的：nn.Sequential是一个pytorch预定义的模型框架模块，它的功能很简单，只是将所有传入的层头尾相连。nn.Linear是一个线性层，也叫全连接层，在tensorflow等框架中名为Dense，而在其他一些网络框架中称为Affine。它的作用就是将所有的输入乘上自己的权重，加上偏差再输出，即y=wx+b。注意Linear有两个参数，第一个参数是输入的大小，第二个参数是输出的大小。我们的线性回归模型只有一个输入和一个输出，所以两个值均为1。由于输入层没有实际运算，我们只需要一个Linear层就能满足运算需求。</p>
<p>定义好结构后，我们还需要初始化网络的参数。初始化算法有很多，我们使用简单的正态分布初始化。初始化函数在torch.nn.init中定义。</p>
<pre><code>def init_weights(m):
	if type(m) == nn.Linear:
		nn.init.normal_(m.weight)
		m.bias.data.fill_(0.01)
	
net.apply(init_weights)
</code></pre>
<p>这里我们定义了一个init_weights函数——这是初始化一个pytorch网络模块的标准做法。其中m参数是网络中的一层，m.weight和m.bias为这一层的权重和偏差，两者均为pytorch张量。nn.init.normal_函数将这层的权重使用正态分布初始化，而m.bias.data.fill_则将bias直接初始化为0.01。net.apply方法将这个初始化函数应用在网络中的每一层上，这样初始化就完成了。</p>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/net.png" srcset="undefined" class="">
<p>在训练前，网络还需要一个损失函数，也就是神经网络判断自己离正确答案有多接近的函数，我们使用比较常用的MSE（均方误差）函数。另外，我们还需要一个梯度下降算法，这里我们使用小批量随机梯度下降算法(SGD)，也就是前文所讲的随机挑选出一小部分样本计算梯度的算法。</p>
<pre><code>criterion = torch.nn.MSELoss() #均方误差损失函数
#SGD算法收集神经网络的所有参数进行学习，lr为学习率
#学习率是一个超参数，也就是神经网络以多快的速度学习
#超参数是神经网络不能自己学习到的参数，需要人来调整
#以前文小球的例子做类比，学习率相当于小球每一步滚多远
optimizer = torch.optim.SGD(net.parameters(),lr=0.01)
</code></pre>
<p>全部准备完毕后，我们终于可以开始训练神经网络了。神经网络通常需要将整个训练集循环使用多次才能达到最佳训练效果，每一次循环被称作一代(epoch)，我们让这个网络循环五代。</p>
<pre><code>for epoch in range(5):
	loss = 0
	for data in dataloader:
		#从dataloader中取数据
		features, labels = data
		#将梯度下降算法的梯度清零，每一个小批量的梯度应该独立
		optimizer.zero_grad()
		#通过正向传播得出当前网络的输出
		outputs = net(features)
		#使用损失函数计算当前输出和真实值（标签）差距
		loss = criterion(outputs,labels)
		#根据算出的损失反向传播
		loss.backward()
		#最后梯度下降算法更新所有参数
		optimizer.step()
	
	#每运行完一代就显示训练效果
	print(&quot;epoch {}, loss {}&quot;.format(epoch+1,loss.item()))
</code></pre>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/train.png" srcset="undefined" class="">
<p>损失不断变小说明网络的输出越来越接近真实值，至此，我们的神经网络就训练成功了。我们来看一看训练的效果。</p>
<pre><code>#准确的线性函数输入和输出
x = [-2,-1.5,-1,-0.5,0,0.5,1,1.5,2]
y = list(map(lambda x:3*x+2,x))
#生成一些随机x值用于测试神经网络
test = torch.randn(size=(50,1))
#测试时不需要计算梯度，torch.no_grad关闭梯度计算
with torch.no_grad():
	#通过正向传播得出网络的输出
	predicted = net(test)
#绘图函数
plt.plot(x, y, 'go', label='True data', alpha=0.5)
plt.plot(test, predicted, '-', label='Predictions', 	alpha=0.5)
plt.legend(loc='best')
plt.show();
</code></pre>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/test.png" srcset="undefined" class="">
<p>可以看到预测值连成的线与实际值相差无几。</p>
<p>我们也可以直接查看网络的参数。</p>
<pre><code>#网络可以通过索引获得其中包含的层
net[0].weight, net[0].bias
</code></pre>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/parameters.png" srcset="undefined" class="">
<p>网络的权重为2.9898，偏差为1.9971，与实际值3和2几乎一致。</p>
<h1 id="使用caffe2构建网络"><a class="markdownIt-Anchor" href="#使用caffe2构建网络"></a> 使用caffe2构建网络</h1>
<p>##caffe2与pytorch的不同</p>
<p>caffe2的神经网络构造方法没有pytorch那么直观，这是caffe2和pytorch的设计理念不同造成的。pytorch注重简单灵活，因此它使用命令式编程，所有函数都会像普通的python函数一样执行，可以直接和python的循环搭配使用。但负面作用是pytorch只能动态生成计算图，不能从计算图上进行全局优化，运行较慢。但最新的pytorch引入了JIT编译器，再加上pytorch可以使用高性能的caffe2加速计算，大大提高了运行效率。</p>
<p>caffe2更加注重效率，因此它包含了大量优化过的操作，并且使用符号式编程，所有的网络中的元素先定义成不同的符号。符号只是个占位符，最后需要编译才能运行。编译时网络编译器可以从全局上优化代码，并生成效率高的静态计算图，但是不能直接使用python提供的一些功能，而是需要使用框架的符号作为替代（在某些网络上，python的循环，条件判断等语句都不能使用，需要用caffe2提供的操作符完成）。因为它编写较为麻烦，大部分caffe2模型都是先在pytorch上构建好然后导出到caffe2上运行。在pytorch1.0发布后，Facebook更是直接将两个框架合二为一，使程序员可以同时享受pytorch的便捷和caffe2的高效。</p>
<p>因此，这部分内容只是给想要了解caffe2的读者一个参考，也是为了让文章更加完整（同时也有我对caffe2的一点私心）。由于caffe2的API已经停止支持，并且官方的发行版没有包含一些caffe2需要的库（这些库在这个示例中不需要，但是在大部分网络中需要），建议初学者只学习pytorch。</p>
<h2 id="使用caffe2构建线性回归网络"><a class="markdownIt-Anchor" href="#使用caffe2构建线性回归网络"></a> 使用caffe2构建线性回归网络</h2>
<p>caffe2的API较为晦涩难懂，且提供多种不同的API，这里我们混合使用API进行实作。本例改编自<a href="https://caffe2.ai/docs/tutorial-MNIST.html" target="_blank" rel="noopener">官网教程</a></p>
<p>首先我们导入caffe2需要的包，并使用caffe2提供的一个便利工具ModelHelper类创建一个名叫train_model的网络</p>
<pre><code>from caffe2.python import brew, core, model_helper, optimizer, workspace, net_drawer
train_model = model_helper.ModelHelper(&quot;train_model&quot;)
</code></pre>
<p>ModelHelper创建的模型内置了两个网络——param_init_net和net。param_init_net用于初始化网络中的参数和常量，而net则是真正用于训练的网络。我们在param_init_net中创建三个符号——weights_gt，bias_gt和ONE。</p>
<pre><code>weights_gt = train_model.param_init_net.GivenTensorFill([],&quot;weight_gt&quot;,shape=[1,1],values=[3.])
bias_gt = train_model.param_init_net.GivenTensorFill([],&quot;bias_gt&quot;,shape=[1],values=[2.])
ONE = train_model.param_init_net.ConstantFill([], &quot;ONE&quot;, shape=[1], value=1.0)
</code></pre>
<p>gt是ground true的缩写，也就是线性函数的真实参数。ONE是在后面更新梯度时需要用到的一个值。这里我们分别调用了param_init_net的GivenTensorFill方法和ConstantFill方法创建了两个张量和一个常数。注意caffe2创建张量与pytorch有诸多不同之处。首先每一个张量都需要命名，这是为了在生成计算图时方便找到对应的张量在图上的位置，其次我们可以尝试打印出这几个变量：</p>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/caffe2_variable.png" srcset="undefined" class="">
<p>可以看到这几个变量并没有实际存储张量，而只是一个引用。caffe2只会在最后实际生成计算图时才会实际创建这些变量。</p>
<p>运行</p>
<pre><code>train_model.param_init_net.Proto()
</code></pre>
<p>可以看到在param_init_net中定义的符号。再次强调这些符号并没有实际生成，caffe2只会存储它们的结构，所有的变量都会在创建计算图时生成。</p>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/caffe2_init_proto.png" srcset="undefined" class="">
<p>在创建完网络需要的常量后，我们在net上创建网络运行时使用的变量</p>
<pre><code>features = train_model.net.GaussianFill([], &quot;features&quot;, mean=0.0, std=1.0, shape=[10, 1], run_once=0)
labels = features.FC([weights_gt,bias_gt],&quot;labels&quot;)	noise = train_model.net.GaussianFill([], &quot;noise&quot;, mean=0.0, std=0.1, shape=[10, 1], run_once=0)
noise_labels = labels.Add(noise,&quot;noise_labels&quot;)
noise_labels = noise_labels.StopGradient([], &quot;noise_labels&quot;)
</code></pre>
<p>这里我们看到caffe2和pytorch又一个不同之处。caffe2的数据生成和计算图融合在一起，而不是像pytorch单独使用DataLoader为网络提供数据。这也是因为静态计算图需要包括网络训练的所有步骤。在这里features是一个大小为(10,1)的张量，里面包含了使用标准正态分布生成的数字。然后在features上调用FC方法（即全连接层，与pytorch的Linear一致）生成对应的标签。最后我们加上噪声，使用labels.Add方法可以将label和noise相加。由于这里的FC层和Add层作用是生成样本，我们不需要反向传播，因此还需要使用StopGradient方法告诉caffe2不需要对此处计算梯度。这样，每一次net运行时，就会生成新的特征和标签用于神经网络训练。</p>
<p>接下来就是定义网络本身。</p>
<pre><code>pred = brew.fc(train_model,features,&quot;pred&quot;,dim_in = 1,dim_out = 1)
dist = train_model.SquaredL2Distance([noise_labels,pred],&quot;dist&quot;)
loss = dist.AveragedLoss([],[&quot;loss&quot;])
</code></pre>
<p>这里我们使用caffe2的一个比较高级的API brew生成一个全连接层。brew可以省去自己创建模型的权重和偏差的麻烦，它会掉用初始化算法对权重和偏差初始化。我们只需要定义输入和输出的数量，在这里都为1。接下来是定义损失函数，这里的损失函数和我们在pytorch中使用的MSELoss一样，平方和也被称作L2范数，也就是SquaredL2Distance中的L2。由于caffe2没有提供MSE函数，我们需要先计算平方和再取平均值。</p>
<p>由于caffe2最终能生成静态图，我们可以用net_drawer模块看到计算图的样子，我们调用这个函数查看一下</p>
<pre><code>from IPython import display
graph = net_drawer.GetPydotGraph(train_model.Proto().op, &quot;train&quot;, rankdir=&quot;LR&quot;)
display.Image(graph.create_png(), width=800)
</code></pre>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/computational_graph.png" srcset="undefined" class="">
<p>可以看到我们创建的bias_gt等变量都已经在图上显示出来了。由于caffe2会自动优化神经网络，相较于pytorch而言debug较难，检查计算图有时会很有帮助。</p>
<p>接下来就是加上梯度下降算法</p>
<pre><code>train_model.AddGradientOperators([loss])
ITER = brew.iter(train_model,&quot;iter&quot;)
LR = train_model.LearningRate(ITER, &quot;LR&quot;, base_lr=0.01, policy=&quot;step&quot;, stepsize=1, gamma=0.999)

for param in train_model.params:
	param_grad = train_model.param_to_grad[param]
	train_model.WeightedSum([param, ONE, param_grad, LR], param)
</code></pre>
<p>我们使用loss计算梯度，ITER是用于追踪迭代情况的变量，而LR设定了梯度下降算法的一些超参数。这里除了之前看到的base_lr是学习率以外，gamma是一个衰减率，我们在这里先不用管。在设定好学习率后，我们还需要设定更新网络参数的算法。这里的算法是<code>param = param + param_grad * LR</code>，param是参数，param_grad是反向传播后计算出的梯度，LR是学习率，即要往梯度的方向走多远。使用for循环可以将网络中的参数一一提取出来，然后使用param_to_grad获取计算图上反向传播梯度的符号，再使用WeightedSum设定更新算法。</p>
<p>最后我们进行实际的训练</p>
<pre><code>workspace.RunNetOnce(train_model.param_init_net)
workspace.CreateNet(train_model.net)
total_iters = 500
loss = [0 for i in range(500)]

for i in range(total_iters):
	workspace.RunNet(train_model.net.Proto().name)
	loss[i] = workspace.FetchBlob('loss')
loss
</code></pre>
<p>我们一共迭代500次，使用lost数组记录损失函数的值，然后显示出来。</p>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/result.png" srcset="undefined" class="">
<p>我们也可以把损失值画出来</p>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/lost_plot.png" srcset="undefined" class="">
<p>最后查看一下训练出的模型的参数</p>
<img src="/2019/11/21/pytorch-caffe2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AD%94%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C/caffe2_parameters.png" srcset="undefined" class="">
<p>可以看到神经网络的参数和实际参数基本一致</p>
<p>到这里我们的第一个神经网络就全部完成了，虽然这个例子很简单，但它囊括了神经网络的各种要素，在下一节我们将构建一个能够识别手写数字的网络，使用到的数据集便是之前提到的MNIST数据集。</p>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">神经网络</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/pytorch">pytorch</a>
                
                  <a class="hover-with-bg" href="/tags/caffe2">caffe2</a>
                
                  <a class="hover-with-bg" href="/tags/%E5%9F%BA%E7%A1%80">基础</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
      <br><br>
      
      
  <div id="vcomments" style="width: 90%; margin: 0 auto;"></div>
  <script defer src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script defer src="//unpkg.com/valine/dist/Valine.min.js"></script>

  <script>
    var notify = 'false' === true;
    var verify = 'true' === true;
    var oldLoad = window.onload;
    window.onload = function () {
      new Valine({
        el: '#vcomments',
        notify: notify,
        verify: verify,
        app_id: "jY5stlz5eNHmsmr1p2wsi53q-MdYXbMMI",
        app_key: "R0kCXXt8GsmjHF9Nug1fSRWp",
        placeholder: "说点什么",
        avatar: "retro",
        meta: ['nick', 'mail', 'link'],
        pageSize: "10",
      });
      oldLoad && oldLoad();
    };
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://valine.js.org" target="_blank" rel="nofollow noopener noopener">comments
      powered by Valine.</a></noscript>


    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>


  <script src="/js/lazyload.js" ></script>



  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "pytorch&caffe2 深度学习的魔法 线性回归网络&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>



  

  
    <!-- KaTeX -->
    <link rel="stylesheet" href="/lib/katex/katex.min.css"  >
  





</body>
</html>
